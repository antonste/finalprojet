{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0885de55-8ca4-46ab-b93a-ac28b8ae0f7b",
   "metadata": {},
   "source": [
    "\n",
    "# Parallel Multiobjective PSO Weighted Average Clustering Algorithm Implementation\n",
    "Based on Ling, Huidong, et al. \"A Parallel Multiobjective PSO Weighted Average Clustering Algorithm Based on Apache Spark.\" Entropy 25.2 (2023): 259.\n",
    "\n",
    "## Algorithm Outline\n",
    "\n",
    " \n",
    "\n",
    "## Adaptations made to suggested implementation of the algorithm:\n",
    "1. \n",
    "\n",
    "## Description of production cluster:\n",
    "\n",
    "\n",
    "\n",
    "Datasets: https://cs.joensuu.fi/sipu/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3ce128-b5b4-4359-b549-2b9a52c18397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/26 20:23:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import split, col, size, trim, lit\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FirstSparkJob\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6202549e-e4c9-4f2a-980f-a374b5e67281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and data Parameters\n",
    "np.random.seed(301191)\n",
    "num_clusters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4644df9-ab52-4735-a04d-8c52ed8446b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([53920., 42968.]),\n",
       " array([52019., 42206.]),\n",
       " array([52570., 42476.]),\n",
       " array([54220., 42081.]),\n",
       " array([54268., 43420.])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A3_DATASET_URL = \"https://cs.joensuu.fi/sipu/datasets/a3.txt\"\n",
    "DATA_FOLDER = \"/home/jovyan/work/data\"\n",
    "A3_LOCAL_PATH = os.path.join(DATA_FOLDER, \"a3.txt\")\n",
    "\n",
    "# Download Data\n",
    "response = requests.get(A3_DATASET_URL)\n",
    "if not os.path.exists(A3_LOCAL_PATH):\n",
    "    with open(A3_LOCAL_PATH, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Load clean data into spark\n",
    "data = sc.textFile(A3_LOCAL_PATH)\n",
    "parsed_data = data.map(lambda row: np.array([float(x) for x in row.strip().split()])).cache()\n",
    "\n",
    "parsed_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc2363-deec-4f1b-814d-07477b814955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters, taken from the paper page 9\n",
    "c1 = c2 = 1.49445    # Learning parameters\n",
    "w = 3                # Weight factor\n",
    "num_particles = 50   # Particles number\n",
    "archive_depth = 15   # Number of items in the archive\n",
    "\n",
    "# Get Data dimensions\n",
    "dimensions = len(parsed_data.take(1)[0])\n",
    "\n",
    "# Particle Initialization\n",
    "def initialize_particles(num_particles, num_clusters, num_features):\n",
    "    particles = []\n",
    "    for _ in range(num_particles):\n",
    "        particle = {\n",
    "            'position': np.random.rand(num_clusters, num_features),\n",
    "            'velocity': np.random.rand(num_clusters, num_features),\n",
    "            'pbest_position': None,\n",
    "            'pbest_value': float('inf')\n",
    "        }\n",
    "        particles.append(particle)\n",
    "    return particles\n",
    "\n",
    "particles = initialize_particles(num_particles, num_clusters, num_features)\n",
    "particles_bc = sc.broadcast(particles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c90c78-651f-4cfe-930b-36acb745a17d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_particles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         particles\u001b[38;5;241m.\u001b[39mappend(particle)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m particles\n\u001b[0;32m---> 14\u001b[0m particles \u001b[38;5;241m=\u001b[39m initialize_particles(\u001b[43mnum_particles\u001b[49m, num_clusters, num_features)\n\u001b[1;32m     15\u001b[0m particles_bc \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(particles)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Fitness Evaluation Function\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_particles' is not defined"
     ]
    }
   ],
   "source": [
    "# Fitness Evaluation Function\n",
    "def compute_fitness(data_partition, particle_position):\n",
    "    fitness_compactness = 0\n",
    "    fitness_connectivity = 0\n",
    "    cluster_assignments = []\n",
    "\n",
    "    for point in data_partition:\n",
    "        distances = [euclidean(point, centroid) for centroid in particle_position]\n",
    "        assigned_cluster = np.argmin(distances)\n",
    "        cluster_assignments.append((assigned_cluster, point))\n",
    "        fitness_compactness += distances[assigned_cluster]\n",
    "\n",
    "    # Connectivity fitness calculation (pairwise distance within clusters)\n",
    "    for i, (cluster, point) in enumerate(cluster_assignments):\n",
    "        if i < len(cluster_assignments) - 1:\n",
    "            next_point = cluster_assignments[i + 1][1]\n",
    "            fitness_connectivity += euclidean(point, next_point)\n",
    "\n",
    "    return fitness_compactness, fitness_connectivity\n",
    "\n",
    "# Local Fitness Calculation in Partitions with K-means\n",
    "def calculate_local_fitness(data_partition, particles_bc: Broadcast):\n",
    "    particles = particles_bc.value\n",
    "    local_fitness = []\n",
    "\n",
    "    # Convert partition to a list to be compatible with KMeans (since it's an RDD partition)\n",
    "    data_partition = list(data_partition)\n",
    "\n",
    "    for particle in particles:\n",
    "        # Using particle's position as the initial cluster centers in K-means\n",
    "        kmeans = KMeans(n_clusters=num_clusters, init=particle['position'], n_init=1, max_iter=10)\n",
    "        \n",
    "        # Fit the KMeans model to the data in this partition\n",
    "        kmeans.fit(data_partition)\n",
    "        \n",
    "        # Calculate compactness (within-cluster distances)\n",
    "        compactness = 0\n",
    "        for i, center in enumerate(kmeans.cluster_centers_):\n",
    "            # Get points assigned to this cluster\n",
    "            points_in_cluster = [point for idx, point in enumerate(data_partition) if kmeans.labels_[idx] == i]\n",
    "            compactness += sum(euclidean(point, center) for point in points_in_cluster)\n",
    "\n",
    "        # Calculate connectivity (pairwise distance within each cluster)\n",
    "        connectivity = 0\n",
    "        for i, point in enumerate(data_partition[:-1]):\n",
    "            # Distance to the next point in the same partition\n",
    "            connectivity += euclidean(point, data_partition[i + 1])\n",
    "\n",
    "        # Append fitness (compactness and connectivity) for this particle in this partition\n",
    "        local_fitness.append((compactness, connectivity))\n",
    "\n",
    "    return local_fitness\n",
    "\n",
    "# Aggregate and Weighted Average Fitness Calculation\n",
    "def aggregate_fitness(local_fitness_rdd, num_samples):\n",
    "    fitness_values = local_fitness_rdd.collect()\n",
    "    weighted_fitness = np.average(fitness_values, axis=0, weights=[partition_size / num_samples for partition_size in [len(f) for f in fitness_values]])\n",
    "    return weighted_fitness\n",
    "\n",
    "# Update Particle Position and Velocity\n",
    "def update_particles(particles, global_best_position):\n",
    "    for particle in particles:\n",
    "        for cluster in range(num_clusters):\n",
    "            inertia = w * particle['velocity'][cluster]\n",
    "            cognitive = c1 * np.random.rand() * (particle['pbest_position'][cluster] - particle['position'][cluster])\n",
    "            social = c2 * np.random.rand() * (global_best_position[cluster] - particle['position'][cluster])\n",
    "            particle['velocity'][cluster] = inertia + cognitive + social\n",
    "            particle['position'][cluster] += particle['velocity'][cluster]\n",
    "\n",
    "    return particles\n",
    "\n",
    "# External Archive and Crowding Distance Calculation\n",
    "def update_archive(particles):\n",
    "    # Use non-dominated sorting or other Pareto-based method to keep top solutions\n",
    "    archive = []\n",
    "    for particle in particles:\n",
    "        archive.append((particle['position'], particle['pbest_value']))\n",
    "    # Sort and prune archive if needed (e.g., keep unique non-dominated solutions)\n",
    "    archive = sorted(archive, key=lambda x: x[1])[:num_particles]\n",
    "    return archive\n",
    "\n",
    "# Main Optimization Loop\n",
    "for iteration in range(num_iterations):\n",
    "    # Local Fitness Calculation\n",
    "    local_fitness_rdd = data_rdd.mapPartitions(lambda partition: calculate_local_fitness(partition, particles_bc))\n",
    "    global_fitness = aggregate_fitness(local_fitness_rdd, num_samples=data_rdd.count())\n",
    "\n",
    "    # Update pbest and gbest\n",
    "    for particle in particles:\n",
    "        if particle['pbest_value'] > global_fitness:\n",
    "            particle['pbest_position'] = particle['position']\n",
    "            particle['pbest_value'] = global_fitness\n",
    "\n",
    "    global_best_position = min(particles, key=lambda p: p['pbest_value'])['pbest_position']\n",
    "    particles = update_particles(particles, global_best_position)\n",
    "\n",
    "    # Broadcast updated particles\n",
    "    particles_bc = sc.broadcast(particles)\n",
    "\n",
    "# Final Archive of Best Solutions\n",
    "archive = update_archive(particles)\n",
    "print(\"Final Archive:\", archive)\n",
    "\n",
    "# Stopping Spark Context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
