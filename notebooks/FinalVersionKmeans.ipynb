{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2973f00-f3b9-432a-a076-a22965441403",
   "metadata": {},
   "source": [
    "# Parallel KMeans implementation\n",
    "Based on J. Y. Q. H. Z. W. a. J. C. Bowen Wang, “Parallelizing K-means-based Clustering on Spark,” International Conference on Advanced Cloud and Big Data, 2016. \n",
    "\n",
    "## Parallel partition-based algorithm outline\n",
    "1. Initialize centroids by randomly selecting k points from the data set. Broadcast selected centroids to all nodes\n",
    "1. While centroids are moving:\n",
    "    1. Broadcast current centroids\n",
    "    1. For each point\n",
    "        1. Compute the distance to all centroids\n",
    "        1. Asign the closest cluster\n",
    "    1. For each cluster\n",
    "        1. Compute local mean\n",
    "    1. Compute the mean for each cluster for each partition\n",
    " \n",
    "\n",
    "## Adaptations made to suggested implementation of the algorithm:\n",
    "1. The authors suggest using SparseVector, with chosen data sets it is better to use regular arrays\n",
    "1. We use the random sample for centroids initialization as described in *Scalable K-Means++* because the quality of initial centroids has a major effect on the quality\n",
    "1. We use crisp clustering only i. e. each point can be a member of one cluster only\n",
    "1. We use Euclidian data because it is the recommended distance function for dense data.\n",
    "\n",
    "## Description of production cluster on Azure:\n",
    "We are using Azure HDIsight in order to run a spark cluster. We are using a cluster with the following configuration:\n",
    "* two master nodes Standard_D12_V2 4 CPU Cores 28GB RAM\n",
    "* eight slave nodes Standard_D13_V2 8 CPU Cores 56GB RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdfeb303-1bd0-431f-a3d2-28976e61e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83db861f-d4bf-47bb-876b-7521b6959d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import split, col, size, trim, lit\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FirstSparkJob\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc909081-dda9-4d35-bece-f8042a2fb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 301191\n",
    "K = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3db894d-cccf-4a1b-a0bc-32217ea92707",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3_DATASET_URL = \"https://cs.joensuu.fi/sipu/datasets/a3.txt\"\n",
    "DATA_FOLDER = \"/home/jovyan/work/data\"\n",
    "A3_LOCAL_PATH = os.path.join(DATA_FOLDER, \"a3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6624026-8d8d-4fa2-b83b-b0e75f8b5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "response = requests.get(A3_DATASET_URL)\n",
    "if not os.path.exists(A3_LOCAL_PATH):\n",
    "    with open(A3_LOCAL_PATH, 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee18c992-3aa5-4e44-8377-4539c97d92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DenseVector([53920.0, 42968.0]),\n",
       " DenseVector([52019.0, 42206.0]),\n",
       " DenseVector([52570.0, 42476.0]),\n",
       " DenseVector([54220.0, 42081.0]),\n",
       " DenseVector([54268.0, 43420.0])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean data into spark\n",
    "data = sc.textFile(A3_LOCAL_PATH)\n",
    "parsed_data = data.map(lambda row: Vectors.dense([float(x) for x in row.strip().split()]))\n",
    "parsed_data.cache()\n",
    "\n",
    "parsed_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68610ba5-a86d-4234-bdee-074e930242c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0\tShifts:28496.909826879117\ttime taken:0.21987652778625488\n",
      "Iteration:1\tShifts:14512.573275107046\ttime taken:0.2187638282775879\n",
      "Iteration:2\tShifts:6404.757819968364\ttime taken:0.21590971946716309\n",
      "Iteration:3\tShifts:3147.2457307404798\ttime taken:0.21690726280212402\n",
      "Iteration:4\tShifts:1799.0314255011751\ttime taken:0.4547231197357178\n",
      "Iteration:5\tShifts:1222.2122357790731\ttime taken:0.21929264068603516\n",
      "Iteration:6\tShifts:1011.5807744583558\ttime taken:0.2185688018798828\n",
      "Iteration:7\tShifts:756.896907787936\ttime taken:0.21982431411743164\n",
      "Iteration:8\tShifts:472.1670675801665\ttime taken:0.2169511318206787\n",
      "Iteration:9\tShifts:253.05295585440612\ttime taken:0.2142651081085205\n",
      "Final centroids: [[19110.20186335 50176.14995563]\n",
      " [23398.7486223  15450.88639254]\n",
      " [50551.12642882 36918.3820575 ]]\n"
     ]
    }
   ],
   "source": [
    "def compute_distance(p, centroid):\n",
    "    return np.sqrt(np.sum((np.array(p) - np.array(centroid))**2))\n",
    "\n",
    "def kmeans(rdd, centroids, max_iters=10, tolerance=0.1):\n",
    "    for i in range(max_iters):\n",
    "        timer = time.time()\n",
    "        \n",
    "        # Broadcast the centroids to all nodes\n",
    "        broadcast_centroids = spark.sparkContext.broadcast(centroids)\n",
    "        clustered_rdd = rdd.map(lambda p: (np.argmin([compute_distance(p, c) for c in broadcast_centroids.value]), (p, 1)))\n",
    "\n",
    "        # Recompute centroids by averaging points in each cluster\n",
    "        new_centroids = (\n",
    "            clustered_rdd\n",
    "            .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))  # Sum points and count\n",
    "            .map(lambda x: (x[0], x[1][0] / x[1][1]))  # Compute new centroids\n",
    "            .collectAsMap()\n",
    "        )\n",
    "\n",
    "        new_centroids_arr = np.array([new_centroids[j] for j in range(len(centroids))], dtype=np.float64)\n",
    "        \n",
    "        # Calculate sum of shifts for each centroid\n",
    "        shifts = np.sum(np.linalg.norm(new_centroids_arr - centroids, axis=1))\n",
    "        \n",
    "        print(f\"Iteration:{i}\\tShifts:{shifts}\\ttime taken:{time.time()-timer}\")\n",
    "        if shifts < tolerance:\n",
    "            break\n",
    "\n",
    "        # Update the centroids\n",
    "        centroids = new_centroids_arr\n",
    "    \n",
    "        # Free memory\n",
    "        broadcast_centroids.unpersist()\n",
    "\n",
    "    return centroids\n",
    "\n",
    "# Run the K-Means with broadcasting\n",
    "centroids = np.array(parsed_data.takeSample(False, K, seed=RANDOM_SEED), dtype=np.float64)\n",
    "final_centroids = kmeans(parsed_data, centroids)\n",
    "print(\"Final centroids:\", final_centroids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
