{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import time\n",
    "import requests\n",
    "\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from itertools import groupby, compress\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "\n",
    "# For reproducability of results\n",
    "RANDOM_SEED = 30111991\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/14 18:30:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import split, col, size, trim, lit\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BetterKMeans\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(v1, v2):\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2, axis=1))\n",
    "\n",
    "def closest_centroid(points, centroids):\n",
    "    distances = np.sqrt(((points[:, None] - centroids[None, :]) ** 2).sum(axis=2))\n",
    "    closest_centroids_indices = np.argmin(distances, axis=1)\n",
    "    return closest_centroids_indices\n",
    "\n",
    "def calc_partition_centroid_means(partition, centroids):\n",
    "    points = np.array(list(partition)) \n",
    "    if len(points) == 0:  # Handle empty partitions\n",
    "        return []\n",
    "\n",
    "    closest_indices = closest_centroid(points, centroids.value)\n",
    "    \n",
    "    # Combine points with their respective closest centroids\n",
    "    data = pd.DataFrame({\n",
    "        \"Centroid\": closest_indices,\n",
    "        \"Point\": list(points)\n",
    "    })\n",
    "\n",
    "    # Expand Point into multiple dimensions\n",
    "    point_df = pd.DataFrame(data['Point'].tolist(), index=data.index)\n",
    "    combined_df = pd.concat([data['Centroid'], point_df], axis=1)\n",
    "\n",
    "    # Group by Centroid and calculate mean for each group\n",
    "    means = combined_df.groupby('Centroid').mean().reset_index().values\n",
    "    return means\n",
    "\n",
    "def closest_centroids(data, centroids):\n",
    "    return data.mapPartitions(lambda partition: [calc_partition_centroid_means(partition, centroids)])\n",
    "\n",
    "def aggregate_means(rdd):\n",
    "    partition_means = np.concatenate(rdd.collect(), axis=0)\n",
    "    num_dimensions = partition_means.shape[1] - 1  # Subtract 1 for the 'Centroid' column    \n",
    "    columns = ['Centroid'] + [f'dim_{i}' for i in range(num_dimensions)]\n",
    "    \n",
    "    df = pd.DataFrame(partition_means, columns=columns)\n",
    "    \n",
    "    # Group by Centroid and calculate the mean for each dimension\n",
    "    new_centroids = df.groupby('Centroid').mean().reset_index().values\n",
    "    return new_centroids\n",
    "\n",
    "def calc_error(new_centroids, old_centroids):\n",
    "    return np.sum(euclidean_dist(new_centroids, old_centroids))\n",
    "\n",
    "def pkmeans(data, n, max_iterations=150, stop_distance=0.001):\n",
    "    print(time.asctime(), \"Started\")\n",
    "    init_centroids = np.array(data.takeSample(False, n, seed=42)) \n",
    "    centroids = sc.broadcast(init_centroids)\n",
    "\n",
    "    iteration = 1\n",
    "    error = float(\"inf\")\n",
    "    while error > stop_distance and iteration <= max_iterations:\n",
    "        loop_start = time.time()\n",
    "        closest_centroids_rdd = closest_centroids(data, centroids)\n",
    "        new_centroids = aggregate_means(closest_centroids_rdd)\n",
    "        error = calc_error(new_centroids[:, 1:], centroids.value)\n",
    "        print(\"{3} Iteration #{0}\\tDistance between old and new centroids: {1:.4f}\\tIteration took: {2:.4f} sec\".format(\n",
    "            iteration, error, time.time() - loop_start, time.asctime()))\n",
    "        \n",
    "        # Update centroids\n",
    "        centroids = sc.broadcast(new_centroids[:, 1:])  # Use the new centroids\n",
    "        iteration += 1\n",
    "\n",
    "    return centroids.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 18:30:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/12/14 18:31:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/12/14 18:31:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/12/14 18:31:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([53920., 42968.]),\n",
       " array([52019., 42206.]),\n",
       " array([52570., 42476.]),\n",
       " array([54220., 42081.]),\n",
       " array([54268., 43420.])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A3_DATASET_URL = \"https://cs.joensuu.fi/sipu/datasets/a3.txt\"\n",
    "DATA_FOLDER = \"/home/jovyan/work/data\"\n",
    "A3_LOCAL_PATH = os.path.join(DATA_FOLDER, \"a3.txt\")\n",
    "\n",
    "# Download Data\n",
    "response = requests.get(A3_DATASET_URL)\n",
    "if not os.path.exists(A3_LOCAL_PATH):\n",
    "    with open(A3_LOCAL_PATH, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Load clean data into spark\n",
    "data = sc.textFile(A3_LOCAL_PATH)\n",
    "parsed_data = data.map(lambda row: array(tuple(map(float, row.strip().split())))).cache()\n",
    "\n",
    "parsed_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 14 19:15:57 2024 Started\n",
      "Sat Dec 14 19:15:57 2024 Iteration #1\tDistance between old and new centroids: 112307.3063\tIteration took: 0.1176 sec\n",
      "Sat Dec 14 19:15:58 2024 Iteration #2\tDistance between old and new centroids: 44519.8717\tIteration took: 0.1157 sec\n",
      "Sat Dec 14 19:15:58 2024 Iteration #3\tDistance between old and new centroids: 31843.3779\tIteration took: 0.1090 sec\n",
      "Sat Dec 14 19:15:58 2024 Iteration #4\tDistance between old and new centroids: 21192.3485\tIteration took: 0.1050 sec\n",
      "Sat Dec 14 19:15:58 2024 Iteration #5\tDistance between old and new centroids: 9646.1662\tIteration took: 0.1113 sec\n",
      "Sat Dec 14 19:15:58 2024 Iteration #6\tDistance between old and new centroids: 6134.1732\tIteration took: 0.1042 sec\n",
      "Sat Dec 14 19:15:58 2024 Iteration #7\tDistance between old and new centroids: 5262.5803\tIteration took: 0.1100 sec\n",
      "Sat Dec 14 19:15:58 2024 Iteration #8\tDistance between old and new centroids: 2496.2124\tIteration took: 0.1101 sec\n",
      "Sat Dec 14 19:15:58 2024 Iteration #9\tDistance between old and new centroids: 1141.0438\tIteration took: 0.1060 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #10\tDistance between old and new centroids: 429.2790\tIteration took: 0.1196 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #11\tDistance between old and new centroids: 442.6902\tIteration took: 0.1107 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #12\tDistance between old and new centroids: 610.2148\tIteration took: 0.1013 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #13\tDistance between old and new centroids: 218.2570\tIteration took: 0.1009 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #14\tDistance between old and new centroids: 225.1043\tIteration took: 0.1101 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #15\tDistance between old and new centroids: 271.3706\tIteration took: 0.1004 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #16\tDistance between old and new centroids: 307.2145\tIteration took: 0.1027 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #17\tDistance between old and new centroids: 177.8418\tIteration took: 0.1071 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #18\tDistance between old and new centroids: 17.9195\tIteration took: 0.1012 sec\n",
      "Sat Dec 14 19:15:59 2024 Iteration #19\tDistance between old and new centroids: 0.0000\tIteration took: 0.1064 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 19:17:56 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "24/12/14 19:17:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "centroids = pkmeans(parsed_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 6.09482470e+03, 4.10024568e+04],\n",
       "       [1.00000000e+00, 2.55802081e+04, 2.80568926e+04],\n",
       "       [2.00000000e+00, 8.98714493e+03, 4.99091014e+04],\n",
       "       [3.00000000e+00, 9.48408850e+03, 3.80838761e+04],\n",
       "       [4.00000000e+00, 4.84747623e+04, 2.38995396e+04],\n",
       "       [5.00000000e+00, 2.79017810e+04, 4.51022025e+04],\n",
       "       [6.00000000e+00, 6.12914925e+04, 4.51113881e+04],\n",
       "       [7.00000000e+00, 9.64706383e+03, 6.13325745e+04],\n",
       "       [8.00000000e+00, 1.75801867e+04, 2.53175267e+04],\n",
       "       [9.00000000e+00, 1.76587190e+04, 5.63090868e+04],\n",
       "       [1.00000000e+01, 2.72420135e+04, 1.06863378e+04],\n",
       "       [1.10000000e+01, 1.12779425e+04, 5.04366897e+04],\n",
       "       [1.20000000e+01, 3.08898481e+04, 1.88458165e+04],\n",
       "       [1.30000000e+01, 4.39303021e+04, 3.24957292e+04],\n",
       "       [1.40000000e+01, 5.76577326e+04, 1.41785382e+04],\n",
       "       [1.50000000e+01, 3.83125836e+04, 4.58775358e+04],\n",
       "       [1.60000000e+01, 3.85681861e+04, 6.57083550e+03],\n",
       "       [1.70000000e+01, 2.91537500e+03, 5.40871250e+04],\n",
       "       [1.80000000e+01, 2.24866813e+04, 5.96427253e+04],\n",
       "       [1.90000000e+01, 5.92547493e+04, 2.88949287e+04],\n",
       "       [2.00000000e+01, 1.36028841e+04, 6.58121951e+03],\n",
       "       [2.10000000e+01, 5.49761896e+04, 3.86486543e+04],\n",
       "       [2.20000000e+01, 1.00467941e+04, 3.31201471e+04],\n",
       "       [2.30000000e+01, 4.58020000e+04, 5.74428634e+04],\n",
       "       [2.40000000e+01, 1.70699148e+04, 4.15134664e+04],\n",
       "       [2.50000000e+01, 1.84409070e+04, 1.04659922e+04],\n",
       "       [2.60000000e+01, 8.34080365e+03, 1.23280776e+04],\n",
       "       [2.70000000e+01, 5.07469065e+04, 8.66296129e+03],\n",
       "       [2.80000000e+01, 3.47398156e+04, 5.87409574e+04],\n",
       "       [2.90000000e+01, 3.97374348e+04, 5.44333478e+04],\n",
       "       [3.00000000e+01, 2.93047278e+04, 5.93494620e+04],\n",
       "       [3.10000000e+01, 3.55003960e+04, 3.65817987e+04],\n",
       "       [3.20000000e+01, 4.06691958e+04, 2.55286083e+04],\n",
       "       [3.30000000e+01, 1.06134128e+04, 4.25571860e+04],\n",
       "       [3.40000000e+01, 5.93533284e+04, 4.60438507e+04],\n",
       "       [3.50000000e+01, 5.19846491e+03, 5.39948070e+04],\n",
       "       [3.60000000e+01, 3.14946744e+04, 8.71766279e+03],\n",
       "       [3.70000000e+01, 4.69001694e+04, 4.55269113e+04],\n",
       "       [3.80000000e+01, 3.31923000e+04, 4.40188000e+04],\n",
       "       [3.90000000e+01, 1.07744175e+04, 5.91360971e+04],\n",
       "       [4.00000000e+01, 4.81549231e+04, 6.02727115e+04],\n",
       "       [4.10000000e+01, 2.55216034e+04, 5.50991552e+04],\n",
       "       [4.20000000e+01, 1.23667612e+04, 3.58549851e+04],\n",
       "       [4.30000000e+01, 2.98696923e+04, 5.29727692e+04],\n",
       "       [4.40000000e+01, 5.08144366e+04, 4.92982042e+04],\n",
       "       [4.50000000e+01, 5.90177213e+04, 4.94766885e+04],\n",
       "       [4.60000000e+01, 4.31400000e+04, 3.57561071e+04],\n",
       "       [4.70000000e+01, 6.65779330e+03, 2.04380615e+04],\n",
       "       [4.80000000e+01, 5.87000198e+04, 5.63222372e+04],\n",
       "       [4.90000000e+01, 8.03428947e+03, 2.80313158e+04]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
