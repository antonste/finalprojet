{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a6d2e6-a9e9-4ed5-8575-bb6a609ed720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42acaf7-11c1-4780-8e7c-feac0471f3f0",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Li, Chen, et al. \"Mux-Kmeans: Multiplex Kmeans for Clustering Large-Scale Data Set.\" *Proceedings of the 5th ACM Workshop on Scientific Cloud Computing (ScienceCloud'14)*, 23â€“27 June 2014, Vancouver, BC, Canada, ACM, 2014, pp. 1-7. https://doi.org/10.1145/2608029.2608033\n",
    "\n",
    "## Adaptation\n",
    "1. The algorithm was converted from map-reduce to spark\n",
    "2. The algorithm was adapted to parallelize all the heavy computations including the cluster assignment and twcv\n",
    "3. The TWCV is calculated in parallel utilizing the rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13a92d-036c-4f02-9ea1-a1a38c43a18b",
   "metadata": {},
   "source": [
    "# TWCV Calculation Function\n",
    "\n",
    "### What is TWCV?\n",
    "The **Total Within-Cluster Variation (TWCV)** is a measure of the clustering quality. It calculates the sum of squared Euclidean distances between each data point and the centroid of its assigned cluster. A lower TWCV indicates better clustering as the points are closer to their respective centroids.\n",
    "\n",
    "### Formula\n",
    "$$\n",
    "\\text{TWCV} = \\sum_{j=1}^K \\sum_{x_i \\in C_j} \\| x_i - c_j \\|^2\n",
    "$$\n",
    "Where:\n",
    "- $( K )$: Number of clusters.\n",
    "- $( C_j)$: Set of data points assigned to the $j$-th cluster.\n",
    "- $( x_i)$: Data point in cluster $( C_j )$.\n",
    "- $( c_j)$: Centroid of the $( j )$-th cluster.\n",
    "- $( | x_i - c_j |^2 )$: Squared Euclidean distance between the data point $( x_i )$ and the centroid $( c_j $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5beccd06-3574-4803-bd4a-eaef70ab24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mux-Kmeans\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# Function for RSDS strategy\n",
    "def rsds(centroids, cluster_points):\n",
    "    new_centroids = []\n",
    "    for cluster in cluster_points:\n",
    "        if len(cluster) > 0:\n",
    "            dense_center = np.mean(cluster, axis=0)\n",
    "            radius = np.min([np.linalg.norm(dense_center - c) for c in centroids])\n",
    "            random_point = dense_center + np.random.uniform(-radius, radius, size=dense_center.shape)\n",
    "            new_centroids.append(random_point)\n",
    "    return new_centroids\n",
    "\n",
    "# Function for ADGP strategy\n",
    "def adgp(centroids):\n",
    "    new_centroids = []\n",
    "    for i, c1 in enumerate(centroids):\n",
    "        for j, c2 in enumerate(centroids):\n",
    "            if i < j:  # Avoid duplicate pairings\n",
    "                midpoint = (c1 + c2) / 2\n",
    "                new_centroids.append(midpoint)\n",
    "    return random.sample(new_centroids, len(centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81b3d4f7-c610-490b-a6d6-b0f1ec08e7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([53769., 43786.]),\n",
       " array([52883., 41365.]),\n",
       " array([54448., 42846.]),\n",
       " array([53358., 40498.]),\n",
       " array([54626., 43461.])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A3_DATASET_URL = \"https://cs.joensuu.fi/sipu/datasets/a3.txt\"\n",
    "DATA_FOLDER = \"/home/jovyan/work/data\"\n",
    "A3_LOCAL_PATH = os.path.join(DATA_FOLDER, \"a3.txt\")\n",
    "\n",
    "k = 50  # Number of clusters\n",
    "s = 8  # Number of centroid groups\n",
    "max_iterations = 10\n",
    "num_partitions = 10\n",
    "\n",
    "\n",
    "# Download Data\n",
    "if not os.path.exists(A3_LOCAL_PATH):\n",
    "    with open(A3_LOCAL_PATH, 'wb') as file:\n",
    "        response = requests.get(A3_DATASET_URL)\n",
    "        file.write(response.content)\n",
    "\n",
    "# Load clean data into spark\n",
    "data = sc.textFile(A3_LOCAL_PATH)\n",
    "parsed_data = data.map(lambda row: np.array([float(x) for x in row.strip().split()])).repartition(num_partitions).cache()\n",
    "\n",
    "parsed_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce628202-caa3-4a42-87a2-994631eeacdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(21405576944054.703)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize multiple centroid groups\n",
    "initial_centroids_groups = [np.random.rand(k, len(parsed_data.take(1)[0])) for _ in range(s)]\n",
    "\n",
    "# Main Mux-Kmeans Loop\n",
    "for iteration in range(max_iterations):\n",
    "    twcv_scores = []\n",
    "    cluster_assignments = []\n",
    "\n",
    "    # Evaluate all centroid groups\n",
    "    for centroids in initial_centroids_groups:\n",
    "        # Broadcust the centroids to reuse in map calculation\n",
    "        centroids_broadcast = sc.broadcast(centroids)\n",
    "        \n",
    "        # Assign points to the nearest centroids\n",
    "        centroid_to_point = parsed_data.map(\n",
    "            lambda point: (\n",
    "                np.argmin([np.linalg.norm(point - c) for c in centroids_broadcast.value]), # cluster_id\n",
    "                point # original point\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate TWCV for the current group\n",
    "        # Row is (cluster_id, point)\n",
    "        twcv = centroid_to_point.map(lambda row: np.linalg.norm(row[1] - centroids_broadcast.value[row[0]]) ** 2).sum()\n",
    "        twcv_scores.append((centroids, twcv))\n",
    "    \n",
    "    # Prune: Retain top-performing centroid groups\n",
    "    twcv_scores.sort(key=lambda x: x[1])  # Sort by TWCV (lower is better)\n",
    "    best_groups = twcv_scores[:s // 2]    # Retain top s/2 groups\n",
    "    \n",
    "    # Permute and Incubate\n",
    "    new_centroids_groups = []\n",
    "    for centroids, _, cluster_points in best_groups:\n",
    "        if iteration % 2 == 0:  # Alternate between RSDS and ADGP\n",
    "            new_centroids = rsds(centroids, cluster_points)\n",
    "        else:\n",
    "            new_centroids = adgp(centroids)\n",
    "        new_centroids_groups.append(new_centroids)\n",
    "    \n",
    "    # Prepare centroid groups for the next iteration\n",
    "    initial_centroids_groups = [group[0] for group in best_groups] + new_centroids_groups\n",
    "\n",
    "# Final Output: Select the best centroid group\n",
    "final_group = min(twcv_scores, key=lambda x: x[1])\n",
    "print(f\"Best TWCV: {final_group[1]}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441ab74-1c04-45c3-8b6d-2f25daa98938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
