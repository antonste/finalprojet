{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae6aff-90a2-4e2d-818a-4ca905dc1dd1",
   "metadata": {},
   "source": [
    "### Download and prepare KDD1999 10% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2987b1-fdaa-4ca8-87f1-5c432c27d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def load_kddcup_data(filepath):\n",
    "    columns = [\n",
    "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "        'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "        'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "    ]\n",
    "    \n",
    "    data = pd.read_csv(filepath, header=None, names=columns)\n",
    "    return data\n",
    "\n",
    "def preprocess_kddcup_data(data):\n",
    "    # Separate features and labels\n",
    "    X = data.drop(columns=['label'])\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Convert all numerical features to standard normal destribution\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            # One hot encode categorical features\n",
    "            # ('cat', OneHotEncoder(), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor.fit_transform(X)\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7f4428-c740-46e3-a92d-f6165411d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "data_file_path = download_kddcup99(url, \"kdd_10_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f448e00f-eccf-474a-a627-d468e438d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15  # Number of clusters\n",
    "max_iterations = 5\n",
    "num_partitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5150c65-d3bb-42d8-bb8c-84ad76a662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_kddcup_data(data_file_path)\n",
    "processed_data = preprocess_kddcup_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a9b854-de85-410d-a438-ba350dff34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "processed_data, y = make_blobs(n_samples=200000, n_features=25, centers=k, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf0e40-fec5-4aec-9a1f-ba9a9ef920e1",
   "metadata": {},
   "source": [
    "### Simple KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f31326d9-2cfc-4cf3-b0d7-3d7fc0baf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "def initialize_centroids(X, k, seed=314):\n",
    "    \"\"\"\n",
    "    Randomly initialize centroids from the dataset with a fixed seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_indices = random.sample(range(len(X)), k)\n",
    "    centroids = [X[idx][:] for idx in random_indices]  # Copy selected points\n",
    "    return centroids\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"Compute the Euclidean distance between two points.\"\"\"\n",
    "    return math.sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(point1, point2)))\n",
    "\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    Compute the distance between each data point and each centroid.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for point in X:\n",
    "        distances.append([euclidean_distance(point, centroid) for centroid in centroids])\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"\n",
    "    Assign each data point to the closest centroid.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for distance_list in distances:\n",
    "        min_distance_index = distance_list.index(min(distance_list))\n",
    "        labels.append(min_distance_index)\n",
    "    return labels\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update centroids as the mean of all points assigned to each cluster.\n",
    "    \"\"\"\n",
    "    centroids = [[0] * len(X[0]) for _ in range(k)]\n",
    "    counts = [0] * k\n",
    "    \n",
    "    # Sum points for each cluster\n",
    "    for idx, label in enumerate(labels):\n",
    "        for dim in range(len(X[idx])):\n",
    "            centroids[label][dim] += X[idx][dim]\n",
    "        counts[label] += 1\n",
    "    \n",
    "    # Divide by the count to compute the mean\n",
    "    for i in range(k):\n",
    "        if counts[i] > 0:  # Avoid division by zero\n",
    "            centroids[i] = [val / counts[i] for val in centroids[i]]\n",
    "    return centroids\n",
    "\n",
    "def has_converged(new_centroids, old_centroids, tolerance):\n",
    "    \"\"\"\n",
    "    Check if centroids have converged based on a given tolerance.\n",
    "    \"\"\"\n",
    "    for nc, oc in zip(new_centroids, old_centroids):\n",
    "        if any(abs(n - o) > tolerance for n, o in zip(nc, oc)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def simple_kmeans(X, k, max_iters=150, tolerance=0):\n",
    "    \"\"\"\n",
    "    Perform the K-Means clustering algorithm.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        iteration_time = time.time()\n",
    "        old_centroids = [c[:] for c in centroids]  # Deep copy\n",
    "        distances = compute_distances(X, centroids)\n",
    "        labels = assign_clusters(distances)\n",
    "        centroids = update_centroids(X, labels, k)\n",
    "        \n",
    "        print(f\"Iteration: {i}\\ttime taken: {time.time() - iteration_time:.4f} seconds\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if has_converged(centroids, old_centroids, tolerance):\n",
    "            print(f\"K-Means converged after {i+1} iterations.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total time: {time.time() - start_time:.4f} seconds\")\n",
    "    return centroids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a3e88f-2c13-400a-877e-c9dacf8b7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\ttime taken: 14.9475 seconds\n",
      "Iteration: 1\ttime taken: 13.6388 seconds\n",
      "Iteration: 2\ttime taken: 13.5315 seconds\n",
      "Iteration: 3\ttime taken: 13.6660 seconds\n",
      "Iteration: 4\ttime taken: 13.6080 seconds\n",
      "Iteration: 5\ttime taken: 13.5686 seconds\n",
      "Iteration: 6\ttime taken: 13.6324 seconds\n",
      "Iteration: 7\ttime taken: 13.5577 seconds\n",
      "Iteration: 8\ttime taken: 13.5366 seconds\n",
      "Iteration: 9\ttime taken: 13.5449 seconds\n",
      "Iteration: 10\ttime taken: 13.6500 seconds\n",
      "Iteration: 11\ttime taken: 13.5445 seconds\n",
      "Iteration: 12\ttime taken: 13.7197 seconds\n",
      "Iteration: 13\ttime taken: 13.9401 seconds\n",
      "Iteration: 14\ttime taken: 13.5913 seconds\n",
      "Iteration: 15\ttime taken: 13.5854 seconds\n",
      "Iteration: 16\ttime taken: 13.5893 seconds\n",
      "Iteration: 17\ttime taken: 13.5277 seconds\n",
      "Iteration: 18\ttime taken: 13.6221 seconds\n",
      "Iteration: 19\ttime taken: 13.5762 seconds\n",
      "Total time: 273.5813 seconds\n"
     ]
    }
   ],
   "source": [
    "centroids, labels = simple_kmeans(processed_data, k, max_iters=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "505bfb8c-0fa4-4a44-bd29-5d44a18f9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac14c71c-8999-4cad-961c-85275902fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/19 13:19:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerClusterApp\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02812042-45b6-477f-8609-0f062735a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_to_spark_df(X, labels):\n",
    "    \"\"\"\n",
    "    Convert NumPy array and cluster labels into a PySpark DataFrame with features and predictions.\n",
    "    \"\"\"\n",
    "    # Convert NumPy data to a list of Rows with features and predictions\n",
    "    rows = [Row(features=Vectors.dense(X[i]), prediction=int(labels[i])) for i in range(len(labels))]\n",
    "    return spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459c6d9c-65b4-4a75-812f-046b2d0fcee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 13:19:54 WARN TaskSetManager: Stage 0 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/19 13:19:56 WARN TaskSetManager: Stage 1 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/19 13:20:00 WARN TaskSetManager: Stage 3 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.680941640132225\n"
     ]
    }
   ],
   "source": [
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d01905-af96-4ea6-b356-dbfd20031044",
   "metadata": {},
   "source": [
    "# PKMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aed7f57-2365-476d-b194-77d883a88e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_data(X, centroids):\n",
    "    labels = []\n",
    "    for point in X:\n",
    "        distances = np.linalg.norm(centroids - point, axis=1)\n",
    "        labels.append(np.argmin(distances))\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42e8b8b5-8deb-494d-9ee0-c25a786985d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import itertools\n",
    "import typing\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from itertools import groupby, compress\n",
    "\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "def euclidean_dist(v1, v2):\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2, axis=1))\n",
    "\n",
    "def closest_centroid(points, centroids):\n",
    "    distances = np.sqrt(((points[:, None] - centroids[None, :]) ** 2).sum(axis=2))\n",
    "    closest_centroids_indices = np.argmin(distances, axis=1)\n",
    "    return closest_centroids_indices\n",
    "\n",
    "def calc_partition_centroid_means(partition, centroids):\n",
    "    points = np.array(list(partition)) \n",
    "    if len(points) == 0:  # Handle empty partitions\n",
    "        return []\n",
    "\n",
    "    closest_indices = closest_centroid(points, centroids.value)\n",
    "    \n",
    "    # Combine points with their respective closest centroids\n",
    "    data = pd.DataFrame({\n",
    "        \"Centroid\": closest_indices,\n",
    "        \"Point\": list(points)\n",
    "    })\n",
    "\n",
    "    # Expand Point into multiple dimensions\n",
    "    point_df = pd.DataFrame(data['Point'].tolist(), index=data.index)\n",
    "    combined_df = pd.concat([data['Centroid'], point_df], axis=1)\n",
    "\n",
    "    # Group by Centroid and calculate sum and count for each group\n",
    "    grouped = combined_df.groupby('Centroid').agg(['sum', 'count'])\n",
    "\n",
    "    grouped.columns = ['_'.join(map(str, col)).strip() for col in grouped.columns.to_flat_index()]\n",
    "    grouped.reset_index(inplace=True)\n",
    "\n",
    "    # Extract sum and count separately\n",
    "    point_sums = grouped[[f'{i}_sum' for i in range(points.shape[1])]].to_numpy()\n",
    "    counts = grouped['0_count'].to_numpy()\n",
    "\n",
    "    # Return 2 array with each row: [Centroid, *points sums, count]\n",
    "    return np.column_stack([grouped['Centroid'].to_numpy(), point_sums, counts])\n",
    "\n",
    "def aggregate_means(rdd):\n",
    "    partition_means = np.concatenate(rdd.collect(), axis=0)\n",
    "\n",
    "    # Extract dimensions and count\n",
    "    num_dimensions = int(partition_means.shape[1] - 2)  # Exclude centroid and count\n",
    "    sums = partition_means[:, 1:1 + num_dimensions]\n",
    "    counts = partition_means[:, -1]\n",
    "\n",
    "    # Aggregate sums and counts by centroid\n",
    "    data = pd.DataFrame(partition_means, columns=['Centroid'] + [f'dim_{i}_sum' for i in range(num_dimensions)] + ['count'])\n",
    "    aggregated = data.groupby('Centroid').sum()\n",
    "    aggregated_sums = aggregated[[f'dim_{i}_sum' for i in range(num_dimensions)]].to_numpy()\n",
    "    aggregated_counts = aggregated['count'].to_numpy()\n",
    "\n",
    "    # Compute weighted averages\n",
    "    new_centroids = aggregated_sums / aggregated_counts[:, None]\n",
    "\n",
    "    return pd.DataFrame(new_centroids, index=aggregated.index)\n",
    "\n",
    "def handle_missing_centroids(aggregated_centroids, old_centroids):\n",
    "    num_centroids = len(old_centroids)\n",
    "    missing_centroids = set(range(num_centroids)) - set(aggregated_centroids.index)\n",
    "    for ix in missing_centroids:\n",
    "        # Copy old centroids to replace the missing\n",
    "        aggregated_centroids.loc[ix] = old_centroids[ix]\n",
    "    return aggregated_centroids.sort_index().to_numpy()\n",
    "\n",
    "def closest_centroids(data, centroids):\n",
    "    return data.mapPartitions(lambda partition: [calc_partition_centroid_means(partition, centroids)])\n",
    "\n",
    "def handle_missing_centorids(aggregated_centroids, old_centroids):\n",
    "    num_centorids = len(old_centroids)\n",
    "    missing_centroids = set(range(num_centorids)) - set(aggregated_centroids.index)\n",
    "    for ix in missing_centroids:\n",
    "        # Copy old centroids to replace the missing\n",
    "        print(\"handled missing\")\n",
    "        aggregated_centroids.loc[ix] = old_centroids[ix]\n",
    "    return aggregated_centroids.sort_index().to_numpy()\n",
    "\n",
    "def calc_error(new_centroids, old_centroids):\n",
    "    return np.sum(euclidean_dist(new_centroids, old_centroids))\n",
    "\n",
    "def pkmeans(data, n, max_iterations=150, stop_distance=0.001):\n",
    "    print(time.asctime(), \"Started\")\n",
    "    start_time = time.time()\n",
    "    init_centroids = np.array(data.takeSample(False, n, seed=42)) \n",
    "    centroids = sc.broadcast(init_centroids)\n",
    "\n",
    "    iteration = 1\n",
    "    error = float(\"inf\")\n",
    "    while error > stop_distance and iteration <= max_iterations:\n",
    "        loop_start = time.time()\n",
    "        closest_centroids_rdd = closest_centroids(data, centroids)\n",
    "        aggregated_centroids = aggregate_means(closest_centroids_rdd)\n",
    "        new_centroids = handle_missing_centorids(aggregated_centroids, centroids.value)\n",
    "        error = calc_error(new_centroids, centroids.value)\n",
    "        print(\"{3} Iteration #{0}\\tDistance between old and new centroids: {1:.4f}\\tIteration took: {2:.4f} sec\".format(\n",
    "            iteration, error, time.time() - loop_start, time.asctime()))\n",
    "        \n",
    "        # Update centroids\n",
    "        centroids = sc.broadcast(new_centroids) \n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"Total time: {time.time() - start_time}\")\n",
    "\n",
    "    return centroids.value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a91ef754-dbfa-4ffe-82ec-103b0d6f0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data.csv\", processed_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "065b785c-0c39-4c20-9fcf-7ec5244d2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PKmeans\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile(\"data.csv\")\n",
    "\n",
    "parsed_data = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "parsed_data = parsed_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2228c870-281a-45b1-8e45-61bea6f9a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 19 13:20:07 2025 Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 19 13:20:11 2025 Iteration #1\tDistance between old and new centroids: 165.1339\tIteration took: 0.9237 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:12 2025 Iteration #2\tDistance between old and new centroids: 55.2437\tIteration took: 0.9337 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:13 2025 Iteration #3\tDistance between old and new centroids: 0.8852\tIteration took: 0.8377 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:14 2025 Iteration #4\tDistance between old and new centroids: 0.5326\tIteration took: 0.8267 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:15 2025 Iteration #5\tDistance between old and new centroids: 0.3521\tIteration took: 0.8158 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:16 2025 Iteration #6\tDistance between old and new centroids: 0.2481\tIteration took: 0.8323 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:16 2025 Iteration #7\tDistance between old and new centroids: 0.2012\tIteration took: 0.8266 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:17 2025 Iteration #8\tDistance between old and new centroids: 0.1576\tIteration took: 0.8275 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:18 2025 Iteration #9\tDistance between old and new centroids: 0.1439\tIteration took: 0.8155 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:19 2025 Iteration #10\tDistance between old and new centroids: 0.1316\tIteration took: 0.8204 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:20 2025 Iteration #11\tDistance between old and new centroids: 0.1272\tIteration took: 0.8303 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:21 2025 Iteration #12\tDistance between old and new centroids: 0.1243\tIteration took: 0.8251 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:21 2025 Iteration #13\tDistance between old and new centroids: 0.1146\tIteration took: 0.8380 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:22 2025 Iteration #14\tDistance between old and new centroids: 0.1054\tIteration took: 0.8260 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:23 2025 Iteration #15\tDistance between old and new centroids: 0.1050\tIteration took: 0.8175 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:24 2025 Iteration #16\tDistance between old and new centroids: 0.0964\tIteration took: 0.8158 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:25 2025 Iteration #17\tDistance between old and new centroids: 0.0837\tIteration took: 0.7919 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:26 2025 Iteration #18\tDistance between old and new centroids: 0.0734\tIteration took: 0.8220 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:26 2025 Iteration #19\tDistance between old and new centroids: 0.0794\tIteration took: 0.8717 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled missing\n",
      "Sun Jan 19 13:20:27 2025 Iteration #20\tDistance between old and new centroids: 0.0782\tIteration took: 0.8230 sec\n",
      "Total time: 20.465823650360107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_centorids = pkmeans(parsed_data, k, max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "148a41c1-00a4-46b7-96a6-d95ba5184b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 13:20:32 WARN TaskSetManager: Stage 22 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/19 13:20:33 WARN TaskSetManager: Stage 23 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/19 13:20:37 WARN TaskSetManager: Stage 25 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 25:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.635128597168241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labels = assign_clusters_to_data(processed_data, final_centorids)\n",
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f0fd64-2b72-4df6-8c70-33d5386a7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity score between two centroid groups.\n",
    "    The similarity is the inverse of the sum of distances between corresponding centroids.\n",
    "    \"\"\"\n",
    "    distances = [np.linalg.norm(c1 - c2) for c1, c2 in zip(group1, group2)]\n",
    "    return 1 / sum(distances) if sum(distances) != 0 else float('inf')\n",
    "\n",
    "\n",
    "def find_most_dissimilar_group(target_group, groups):\n",
    "    \"\"\"\n",
    "    Find the most dissimilar group to the target group.\n",
    "    \"\"\"\n",
    "    max_dissimilarity = -float('inf')\n",
    "    most_dissimilar = None\n",
    "    \n",
    "    for group in groups:\n",
    "        if np.array_equal(target_group, group):\n",
    "            continue  # Skip self-comparison\n",
    "        \n",
    "        similarity = calculate_similarity(target_group, group)\n",
    "        if similarity > max_dissimilarity:\n",
    "            max_dissimilarity = similarity\n",
    "            most_dissimilar = group\n",
    "    \n",
    "    return most_dissimilar\n",
    "\n",
    "\n",
    "def adgp(groups):\n",
    "    \"\"\"\n",
    "    Generate new centroid groups using the Average of Dissimilar Group Pairs (ADGP).\n",
    "    \"\"\"\n",
    "    new_groups = []\n",
    "    group_count = len(groups)\n",
    "    \n",
    "    for i, group1 in enumerate(groups):\n",
    "        group2 = find_most_dissimilar_group(group1, groups)\n",
    "        \n",
    "        # Compute the average of corresponding centroids to form a new group\n",
    "        new_group = [(c1 + c2) / 2 for c1, c2 in zip(group1, group2)]\n",
    "        new_groups.append(new_group)\n",
    "    \n",
    "    return new_groups\n",
    "\n",
    "# Permutation function to align centroids across groups\n",
    "def permute_centroids(centroid_groups):\n",
    "    base_group = centroid_groups[0]\n",
    "    permuted_groups = []\n",
    "\n",
    "    for group in centroid_groups[1:]:\n",
    "        # Track matched indices to prevent duplication\n",
    "        matched = set()\n",
    "        permuted_group = []\n",
    "        for base_c in base_group:\n",
    "            # Find the closest unmatched centroid\n",
    "            distances = [(i, np.linalg.norm(base_c - c)) for i, c in enumerate(group) if i not in matched]\n",
    "            if distances:\n",
    "                closest_idx = min(distances, key=lambda x: x[1])[0]\n",
    "                permuted_group.append(group[closest_idx])\n",
    "                matched.add(closest_idx)\n",
    "            else:\n",
    "                # Handle unmatched cases by using a default\n",
    "                permuted_group.append(base_c)\n",
    "        permuted_groups.append(permuted_group)\n",
    "    return [base_group] + permuted_groups\n",
    "\n",
    "def initialize_centroid_groups(parsed_data, k, s):\n",
    "    # Collect a small subset of the data for initialization\n",
    "    sample_data = parsed_data.takeSample(False, k * s, seed=1)\n",
    "    centroid_groups = [\n",
    "        sample_data[i * k:(i + 1) * k] for i in range(s)\n",
    "    ]\n",
    "    return np.array(centroid_groups)\n",
    "\n",
    "def mux_kmeans(data, k, s, max_iterations=20):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize centroid groups\n",
    "    initial_centroids_groups = initialize_centroid_groups(data, k, s)\n",
    "    \n",
    "    # Mux-Kmeans main loop\n",
    "    for iteration in range(max_iterations):\n",
    "        iteration_time = time.time()\n",
    "        twcv_scores = []\n",
    "        \n",
    "        # Evaluate centroid groups\n",
    "        for centroids in initial_centroids_groups:\n",
    "            centroids_broadcast = sc.broadcast(centroids)\n",
    "    \n",
    "            # Assign points to clusters\n",
    "            clustered_rdd = data.map(\n",
    "                lambda p: (\n",
    "                    np.argmin([np.linalg.norm(np.subtract(p, c)) for c in centroids_broadcast.value]),\n",
    "                    (p, 1)\n",
    "                )\n",
    "            ) # (Cluster index, (point, 1))\n",
    "\n",
    "            # Recalculate centroids\n",
    "            new_centroids = (\n",
    "                clustered_rdd\n",
    "                .reduceByKey(lambda x, y: (np.add(x[0], y[0]), x[1] + y[1]))  # Sum points and count\n",
    "                .map(lambda x: (x[0], np.array(x[1][0]) / x[1][1]))  # Compute new centroids\n",
    "                .collectAsMap()\n",
    "            )\n",
    "\n",
    "            new_centroids_arr = np.array(\n",
    "                    [new_centroids[j] if j in new_centroids else centroids[j] for j in range(len(centroids))]\n",
    "            )\n",
    "    \n",
    "            # Update the centroids to the new centroids\n",
    "            centroids_broadcast = sc.broadcast(new_centroids_arr)\n",
    "            \n",
    "            # Assign points to the nearest new centroids\n",
    "            centroid_to_point = data.map(\n",
    "                lambda point: (\n",
    "                    np.argmin([np.linalg.norm(point - c) for c in centroids_broadcast.value]), # cluster_id\n",
    "                    point # original point\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            # Calculate TWCV for the current group\n",
    "            # Row is (cluster_id, point)\n",
    "            twcv = centroid_to_point.map(lambda row: np.linalg.norm(row[1] - centroids_broadcast.value[row[0]]) ** 2).sum()\n",
    "            twcv_scores.append((new_centroids_arr, twcv))\n",
    "    \n",
    "        # Prune and incubate\n",
    "        twcv_scores.sort(key=lambda x: x[1])\n",
    "        best_groups = twcv_scores[:s // 2]\n",
    "        best_centroids = [x[0] for x in best_groups]\n",
    "    \n",
    "        # Permutation\n",
    "        aligned_centroid_groups = permute_centroids(best_centroids)\n",
    "        \n",
    "        # Incubate new groups\n",
    "        new_centroid_groups = []\n",
    "        for group in aligned_centroid_groups:\n",
    "            new_centroid_groups.append(adgp(group))\n",
    "    \n",
    "        # Prepare for the next iteration\n",
    "        initial_centroids_groups = best_centroids + new_centroid_groups\n",
    "        \n",
    "        best_twcv = min(twcv_scores, key=lambda x: x[1])[1]\n",
    "\n",
    "        print(f\"Iteration: {iteration}\\ttime taken: {time.time() - iteration_time:.4f} seconds\\tBest TWCV: {best_twcv}\")\n",
    "\n",
    "    # Select the final best group\n",
    "    final_group = min(twcv_scores, key=lambda x: x[1])\n",
    "    print(f\"Total time: {time.time() - start_time:.4f} seconds\")\n",
    "    return final_group[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bac5187e-16f4-45d1-a1cb-78b79cc4e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 13:20:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\ttime taken: 30.9296 seconds\tBest TWCV: 30782508.941039987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\ttime taken: 32.2628 seconds\tBest TWCV: 23770604.203099255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2\ttime taken: 32.2918 seconds\tBest TWCV: 19155204.29884699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3\ttime taken: 31.9060 seconds\tBest TWCV: 19154543.21682926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4\ttime taken: 32.2171 seconds\tBest TWCV: 19154218.60887158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5\ttime taken: 32.1407 seconds\tBest TWCV: 19154044.40664442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6\ttime taken: 31.8300 seconds\tBest TWCV: 19153936.136977784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7\ttime taken: 31.8246 seconds\tBest TWCV: 19153862.955918346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8\ttime taken: 33.3750 seconds\tBest TWCV: 19153807.50986476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9\ttime taken: 33.6592 seconds\tBest TWCV: 19153759.43172907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\ttime taken: 31.7099 seconds\tBest TWCV: 19153717.12043323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11\ttime taken: 31.7792 seconds\tBest TWCV: 19153683.931395236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12\ttime taken: 31.7384 seconds\tBest TWCV: 19153655.88702098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13\ttime taken: 31.7223 seconds\tBest TWCV: 19153631.533431865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14\ttime taken: 31.8074 seconds\tBest TWCV: 19153610.703134205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15\ttime taken: 32.0326 seconds\tBest TWCV: 19153592.75889112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16\ttime taken: 31.8034 seconds\tBest TWCV: 19153576.651242007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17\ttime taken: 31.6155 seconds\tBest TWCV: 19153562.616959266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18\ttime taken: 3138.1754 seconds\tBest TWCV: 19153549.122602116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19\ttime taken: 55.4126 seconds\tBest TWCV: 19153538.682184897\n",
      "Total time: 3771.2822 seconds\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mux-KMeans\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile(\"data.csv\")\n",
    "\n",
    "parsed_data = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "parsed_data = parsed_data.cache()\n",
    "final_centorids = mux_kmeans(parsed_data, k, s=6)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b85db287-9815-409a-9ae1-f3dcea27fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 14:23:36 WARN TaskSetManager: Stage 0 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/19 14:23:37 WARN TaskSetManager: Stage 1 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/19 14:23:41 WARN TaskSetManager: Stage 3 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.732183963349354\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mux-KMeans-Silhouette\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "labels = assign_clusters_to_data(processed_data, final_centorids)\n",
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e97d5ec-6cd5-436b-823b-654606bb5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "dropdown_algorithm = widgets.Dropdown(\n",
    "    options=['KMeans', 'KMeans', 'Mux-Kmeans'],\n",
    "    value='KMeans',\n",
    "    description='Select Algorithm:',\n",
    "    style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'}\n",
    ")\n",
    "\n",
    "dropdown_dataset = widgets.Dropdown(\n",
    "    options=['KDD1999-10%', 'Syntetic200K-5', 'Syntetic200K-15'],\n",
    "    value='Syntetic200K-5',\n",
    "    description='Select Dataset:',\n",
    "    style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'}\n",
    ")\n",
    "\n",
    "# Sliders\n",
    "k_slider = widgets.IntSlider(value=5, min=5, max=15, step=1, description='K:', style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'})\n",
    "max_iterations_slider = widgets.IntSlider(value=5, min=5, max=20, step=1, description='Max Iterations:',style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'})\n",
    "s_slider = widgets.IntSlider(value=6, min=4, max=10, step=1, description='S:', style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'})\n",
    "\n",
    "# Container to display sliders\n",
    "slider_container = widgets.VBox([k_slider, max_iterations_slider])\n",
    "\n",
    "# Output area for results\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Update function to handle conditional slider\n",
    "def update_sliders(change):\n",
    "    if change['new'] == 'Mux-Kmeans':\n",
    "        slider_container.children = [k_slider, max_iterations_slider, s_slider]\n",
    "    else:\n",
    "        slider_container.children = [k_slider, max_iterations_slider]\n",
    "\n",
    "# Function to execute and display results\n",
    "def execute_action(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        result = {\n",
    "            'Selected Option': dropdown.value,\n",
    "            'Slider 1 Value': slider1.value,\n",
    "            'Slider 2 Value': slider2.value,\n",
    "            'Slider 3 Value': slider3.value,\n",
    "        }\n",
    "        if dropdown.value == 'Option 3':\n",
    "            result['Conditional Slider Value'] = slider4.value\n",
    "        print(\"Results:\", result)\n",
    "\n",
    "# Observe dropdown changes\n",
    "dropdown_algorithm.observe(update_sliders, names='value')\n",
    "\n",
    "# Execute button\n",
    "execute_button = widgets.Button(description='Execute')\n",
    "execute_button.on_click(execute_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9536d710-107e-4fff-a5c9-100a0b288403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f5082c13ce40469911c779afa71e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Algorithm:', layout=Layout(width='300px'), options=('KMeans', 'KMe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widgets.VBox([dropdown_algorithm, dropdown_dataset, slider_container, execute_button, output_area]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
