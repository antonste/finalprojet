{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae6aff-90a2-4e2d-818a-4ca905dc1dd1",
   "metadata": {},
   "source": [
    "### Download and prepare KDD1999 10% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2987b1-fdaa-4ca8-87f1-5c432c27d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def load_kddcup_data(filepath):\n",
    "    columns = [\n",
    "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "        'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "        'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "    ]\n",
    "    \n",
    "    data = pd.read_csv(filepath, header=None, names=columns)\n",
    "    return data\n",
    "\n",
    "def preprocess_kddcup_data(data):\n",
    "    # Separate features and labels\n",
    "    X = data.drop(columns=['label'])\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Convert all numerical features to standard normal destribution\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            # One hot encode categorical features\n",
    "            # ('cat', OneHotEncoder(), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor.fit_transform(X)\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7f4428-c740-46e3-a92d-f6165411d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "data_file_path = download_kddcup99(url, \"kdd_10_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5150c65-d3bb-42d8-bb8c-84ad76a662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_kddcup_data(data_file_path)\n",
    "processed_data_kdd = preprocess_kddcup_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210df508-d990-4c91-9336-a1f9e589b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "processed_data_5, y = make_blobs(n_samples=200000, n_features=25, centers=5, random_state=42)\n",
    "processed_data_15, y = make_blobs(n_samples=200000, n_features=25, centers=15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf0e40-fec5-4aec-9a1f-ba9a9ef920e1",
   "metadata": {},
   "source": [
    "### Simple KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f31326d9-2cfc-4cf3-b0d7-3d7fc0baf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def initialize_centroids(X, k, seed=314):\n",
    "    \"\"\"\n",
    "    Randomly initialize centroids from the dataset with a fixed seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_indices = random.sample(range(len(X)), k)\n",
    "    centroids = [X[idx][:] for idx in random_indices]  # Copy selected points\n",
    "    return centroids\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"Compute the Euclidean distance between two points.\"\"\"\n",
    "    return math.sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(point1, point2)))\n",
    "\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    Compute the distance between each data point and each centroid.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for point in X:\n",
    "        distances.append([euclidean_distance(point, centroid) for centroid in centroids])\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"\n",
    "    Assign each data point to the closest centroid.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for distance_list in distances:\n",
    "        min_distance_index = distance_list.index(min(distance_list))\n",
    "        labels.append(min_distance_index)\n",
    "    return labels\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update centroids as the mean of all points assigned to each cluster.\n",
    "    \"\"\"\n",
    "    centroids = [[0] * len(X[0]) for _ in range(k)]\n",
    "    counts = [0] * k\n",
    "    \n",
    "    # Sum points for each cluster\n",
    "    for idx, label in enumerate(labels):\n",
    "        for dim in range(len(X[idx])):\n",
    "            centroids[label][dim] += X[idx][dim]\n",
    "        counts[label] += 1\n",
    "    \n",
    "    # Divide by the count to compute the mean\n",
    "    for i in range(k):\n",
    "        if counts[i] > 0:  # Avoid division by zero\n",
    "            centroids[i] = [val / counts[i] for val in centroids[i]]\n",
    "    return centroids\n",
    "\n",
    "def has_converged(new_centroids, old_centroids, tolerance):\n",
    "    \"\"\"\n",
    "    Check if centroids have converged based on a given tolerance.\n",
    "    \"\"\"\n",
    "    for nc, oc in zip(new_centroids, old_centroids):\n",
    "        if any(abs(n - o) > tolerance for n, o in zip(nc, oc)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def simple_kmeans(X, k, max_iters=150, tolerance=0):\n",
    "    \"\"\"\n",
    "    Perform the K-Means clustering algorithm.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        iteration_time = time.time()\n",
    "        old_centroids = [c[:] for c in centroids]  # Deep copy\n",
    "        distances = compute_distances(X, centroids)\n",
    "        labels = assign_clusters(distances)\n",
    "        centroids = update_centroids(X, labels, k)\n",
    "        error = np.linalg.norm(np.array(centroids) - np.array(old_centroids))\n",
    "        \n",
    "        print(f\"Iteration: {i}\\ttime taken: {time.time() - iteration_time:.4f} seconds\\tDistance between centroids: {error}\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if has_converged(centroids, old_centroids, tolerance):\n",
    "            print(f\"K-Means converged after {i+1} iterations.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total time: {time.time() - start_time:.4f} seconds\")\n",
    "    return centroids, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d01905-af96-4ea6-b356-dbfd20031044",
   "metadata": {},
   "source": [
    "# PKMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42e8b8b5-8deb-494d-9ee0-c25a786985d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import itertools\n",
    "import typing\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from itertools import groupby, compress\n",
    "\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "def euclidean_dist(v1, v2):\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2, axis=1))\n",
    "\n",
    "def closest_centroid(points, centroids):\n",
    "    distances = np.sqrt(((points[:, None] - centroids[None, :]) ** 2).sum(axis=2))\n",
    "    closest_centroids_indices = np.argmin(distances, axis=1)\n",
    "    return closest_centroids_indices\n",
    "\n",
    "def calc_partition_centroid_means(partition, centroids):\n",
    "    points = np.array(list(partition)) \n",
    "    if len(points) == 0:  # Handle empty partitions\n",
    "        return []\n",
    "\n",
    "    closest_indices = closest_centroid(points, centroids.value)\n",
    "    \n",
    "    # Combine points with their respective closest centroids\n",
    "    data = pd.DataFrame({\n",
    "        \"Centroid\": closest_indices,\n",
    "        \"Point\": list(points)\n",
    "    })\n",
    "\n",
    "    # Expand Point into multiple dimensions\n",
    "    point_df = pd.DataFrame(data['Point'].tolist(), index=data.index)\n",
    "    combined_df = pd.concat([data['Centroid'], point_df], axis=1)\n",
    "\n",
    "    # Group by Centroid and calculate sum and count for each group\n",
    "    grouped = combined_df.groupby('Centroid').agg(['sum', 'count'])\n",
    "\n",
    "    grouped.columns = ['_'.join(map(str, col)).strip() for col in grouped.columns.to_flat_index()]\n",
    "    grouped.reset_index(inplace=True)\n",
    "\n",
    "    # Extract sum and count separately\n",
    "    point_sums = grouped[[f'{i}_sum' for i in range(points.shape[1])]].to_numpy()\n",
    "    counts = grouped['0_count'].to_numpy()\n",
    "\n",
    "    # Return 2 array with each row: [Centroid, *points sums, count]\n",
    "    return np.column_stack([grouped['Centroid'].to_numpy(), point_sums, counts])\n",
    "\n",
    "def aggregate_means(rdd):\n",
    "    partition_means = np.concatenate(rdd.collect(), axis=0)\n",
    "\n",
    "    # Extract dimensions and count\n",
    "    num_dimensions = int(partition_means.shape[1] - 2)  # Exclude centroid and count\n",
    "    sums = partition_means[:, 1:1 + num_dimensions]\n",
    "    counts = partition_means[:, -1]\n",
    "\n",
    "    # Aggregate sums and counts by centroid\n",
    "    data = pd.DataFrame(partition_means, columns=['Centroid'] + [f'dim_{i}_sum' for i in range(num_dimensions)] + ['count'])\n",
    "    aggregated = data.groupby('Centroid').sum()\n",
    "    aggregated_sums = aggregated[[f'dim_{i}_sum' for i in range(num_dimensions)]].to_numpy()\n",
    "    aggregated_counts = aggregated['count'].to_numpy()\n",
    "\n",
    "    # Compute weighted averages\n",
    "    new_centroids = aggregated_sums / aggregated_counts[:, None]\n",
    "\n",
    "    return pd.DataFrame(new_centroids, index=aggregated.index)\n",
    "\n",
    "def handle_missing_centroids(aggregated_centroids, old_centroids):\n",
    "    num_centroids = len(old_centroids)\n",
    "    missing_centroids = set(range(num_centroids)) - set(aggregated_centroids.index)\n",
    "    for ix in missing_centroids:\n",
    "        # Copy old centroids to replace the missing\n",
    "        aggregated_centroids.loc[ix] = old_centroids[ix]\n",
    "    return aggregated_centroids.sort_index().to_numpy()\n",
    "\n",
    "def closest_centroids(data, centroids):\n",
    "    return data.mapPartitions(lambda partition: [calc_partition_centroid_means(partition, centroids)])\n",
    "\n",
    "def handle_missing_centorids(aggregated_centroids, old_centroids):\n",
    "    num_centorids = len(old_centroids)\n",
    "    missing_centroids = set(range(num_centorids)) - set(aggregated_centroids.index)\n",
    "    for ix in missing_centroids:\n",
    "        # Copy old centroids to replace the missing\n",
    "        print(\"handled missing\")\n",
    "        aggregated_centroids.loc[ix] = old_centroids[ix]\n",
    "    return aggregated_centroids.sort_index().to_numpy()\n",
    "\n",
    "def calc_error(new_centroids, old_centroids):\n",
    "    return np.sum(euclidean_dist(new_centroids, old_centroids))\n",
    "\n",
    "def pkmeans(sc, data, k, max_iterations=150, stop_distance=0.001):\n",
    "    print(time.asctime(), \"Started\")\n",
    "    start_time = time.time()\n",
    "    init_centroids = np.array(data.takeSample(False, k, seed=42)) \n",
    "    centroids = sc.broadcast(init_centroids)\n",
    "\n",
    "    iteration = 1\n",
    "    error = float(\"inf\")\n",
    "    while error > stop_distance and iteration <= max_iterations:\n",
    "        loop_start = time.time()\n",
    "        closest_centroids_rdd = closest_centroids(data, centroids)\n",
    "        aggregated_centroids = aggregate_means(closest_centroids_rdd)\n",
    "        new_centroids = handle_missing_centorids(aggregated_centroids, centroids.value)\n",
    "        error = calc_error(new_centroids, centroids.value)\n",
    "        print(\"{3} Iteration #{0}\\tDistance between old and new centroids: {1:.4f}\\tIteration took: {2:.4f} sec\".format(\n",
    "            iteration, error, time.time() - loop_start, time.asctime()))\n",
    "        \n",
    "        # Update centroids\n",
    "        centroids = sc.broadcast(new_centroids) \n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"Total time: {time.time() - start_time}\")\n",
    "\n",
    "    return centroids.value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5f0fd64-2b72-4df6-8c70-33d5386a7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity score between two centroid groups.\n",
    "    The similarity is the inverse of the sum of distances between corresponding centroids.\n",
    "    \"\"\"\n",
    "    distances = [np.linalg.norm(c1 - c2) for c1, c2 in zip(group1, group2)]\n",
    "    return 1 / sum(distances) if sum(distances) != 0 else float('inf')\n",
    "\n",
    "\n",
    "def find_most_dissimilar_group(target_group, groups):\n",
    "    \"\"\"\n",
    "    Find the most dissimilar group to the target group.\n",
    "    \"\"\"\n",
    "    max_dissimilarity = -float('inf')\n",
    "    most_dissimilar = None\n",
    "    \n",
    "    for group in groups:\n",
    "        if np.array_equal(target_group, group):\n",
    "            continue  # Skip self-comparison\n",
    "        \n",
    "        similarity = calculate_similarity(target_group, group)\n",
    "        if similarity > max_dissimilarity:\n",
    "            max_dissimilarity = similarity\n",
    "            most_dissimilar = group\n",
    "    \n",
    "    return most_dissimilar\n",
    "\n",
    "\n",
    "def adgp(groups):\n",
    "    \"\"\"\n",
    "    Generate new centroid groups using the Average of Dissimilar Group Pairs (ADGP).\n",
    "    \"\"\"\n",
    "    new_groups = []\n",
    "    group_count = len(groups)\n",
    "    \n",
    "    for i, group1 in enumerate(groups):\n",
    "        group2 = find_most_dissimilar_group(group1, groups)\n",
    "        \n",
    "        # Compute the average of corresponding centroids to form a new group\n",
    "        new_group = [(c1 + c2) / 2 for c1, c2 in zip(group1, group2)]\n",
    "        new_groups.append(new_group)\n",
    "    \n",
    "    return new_groups\n",
    "\n",
    "# Permutation function to align centroids across groups\n",
    "def permute_centroids(centroid_groups):\n",
    "    base_group = centroid_groups[0]\n",
    "    permuted_groups = []\n",
    "\n",
    "    for group in centroid_groups[1:]:\n",
    "        # Track matched indices to prevent duplication\n",
    "        matched = set()\n",
    "        permuted_group = []\n",
    "        for base_c in base_group:\n",
    "            # Find the closest unmatched centroid\n",
    "            distances = [(i, np.linalg.norm(base_c - c)) for i, c in enumerate(group) if i not in matched]\n",
    "            if distances:\n",
    "                closest_idx = min(distances, key=lambda x: x[1])[0]\n",
    "                permuted_group.append(group[closest_idx])\n",
    "                matched.add(closest_idx)\n",
    "            else:\n",
    "                # Handle unmatched cases by using a default\n",
    "                permuted_group.append(base_c)\n",
    "        permuted_groups.append(permuted_group)\n",
    "    return [base_group] + permuted_groups\n",
    "\n",
    "def initialize_centroid_groups(parsed_data, k, s):\n",
    "    # Collect a small subset of the data for initialization\n",
    "    sample_data = parsed_data.takeSample(False, k * s, seed=1)\n",
    "    centroid_groups = [\n",
    "        sample_data[i * k:(i + 1) * k] for i in range(s)\n",
    "    ]\n",
    "    return np.array(centroid_groups)\n",
    "\n",
    "def mux_kmeans(sc, data, k, s, max_iterations=20):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize centroid groups\n",
    "    initial_centroids_groups = initialize_centroid_groups(data, k, s)\n",
    "    \n",
    "    # Mux-Kmeans main loop\n",
    "    for iteration in range(max_iterations):\n",
    "        iteration_time = time.time()\n",
    "        twcv_scores = []\n",
    "        \n",
    "        # Evaluate centroid groups\n",
    "        for centroids in initial_centroids_groups:\n",
    "            centroids_broadcast = sc.broadcast(centroids)\n",
    "    \n",
    "            # Assign points to clusters\n",
    "            clustered_rdd = data.map(\n",
    "                lambda p: (\n",
    "                    np.argmin([np.linalg.norm(np.subtract(p, c)) for c in centroids_broadcast.value]),\n",
    "                    (p, 1)\n",
    "                )\n",
    "            ) # (Cluster index, (point, 1))\n",
    "\n",
    "            # Recalculate centroids\n",
    "            new_centroids = (\n",
    "                clustered_rdd\n",
    "                .reduceByKey(lambda x, y: (np.add(x[0], y[0]), x[1] + y[1]))  # Sum points and count\n",
    "                .map(lambda x: (x[0], np.array(x[1][0]) / x[1][1]))  # Compute new centroids\n",
    "                .collectAsMap()\n",
    "            )\n",
    "\n",
    "            new_centroids_arr = np.array(\n",
    "                    [new_centroids[j] if j in new_centroids else centroids[j] for j in range(len(centroids))]\n",
    "            )\n",
    "    \n",
    "            # Update the centroids to the new centroids\n",
    "            centroids_broadcast = sc.broadcast(new_centroids_arr)\n",
    "            \n",
    "            # Assign points to the nearest new centroids\n",
    "            centroid_to_point = data.map(\n",
    "                lambda point: (\n",
    "                    np.argmin([np.linalg.norm(point - c) for c in centroids_broadcast.value]), # cluster_id\n",
    "                    point # original point\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            # Calculate TWCV for the current group\n",
    "            # Row is (cluster_id, point)\n",
    "            twcv = centroid_to_point.map(lambda row: np.linalg.norm(row[1] - centroids_broadcast.value[row[0]]) ** 2).sum()\n",
    "            twcv_scores.append((new_centroids_arr, twcv))\n",
    "    \n",
    "        # Prune and incubate\n",
    "        twcv_scores.sort(key=lambda x: x[1])\n",
    "        best_groups = twcv_scores[:s // 2]\n",
    "        best_centroids = [x[0] for x in best_groups]\n",
    "    \n",
    "        # Permutation\n",
    "        aligned_centroid_groups = permute_centroids(best_centroids)\n",
    "        \n",
    "        # Incubate new groups\n",
    "        new_centroid_groups = []\n",
    "        for group in aligned_centroid_groups:\n",
    "            new_centroid_groups.append(adgp(group))\n",
    "    \n",
    "        # Prepare for the next iteration\n",
    "        initial_centroids_groups = best_centroids + new_centroid_groups\n",
    "        \n",
    "        best_twcv = min(twcv_scores, key=lambda x: x[1])[1]\n",
    "\n",
    "        print(f\"Iteration: {iteration}\\ttime taken: {time.time() - iteration_time:.4f} seconds\\tBest TWCV: {best_twcv}\")\n",
    "\n",
    "    # Select the final best group\n",
    "    final_group = min(twcv_scores, key=lambda x: x[1])\n",
    "    print(f\"Total time: {time.time() - start_time:.4f} seconds\")\n",
    "    return final_group[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711dd127-f053-4ee1-a234-83d6981843c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def assign_clusters_to_data(X, centroids):\n",
    "    labels = []\n",
    "    for point in X:\n",
    "        distances = np.linalg.norm(centroids - point, axis=1)\n",
    "        labels.append(np.argmin(distances))\n",
    "    return np.array(labels)\n",
    "\n",
    "def kmeans_to_spark_df(X, labels, spark):\n",
    "    \"\"\"\n",
    "    Convert NumPy array and cluster labels into a PySpark DataFrame with features and predictions.\n",
    "    \"\"\"\n",
    "    rows = [Row(features=Vectors.dense(X[i]), prediction=int(labels[i])) for i in range(len(labels))]\n",
    "    return spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e97d5ec-6cd5-436b-823b-654606bb5edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abb029105cb4c18a11a6c3d80ad86e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Algorithm:', layout=Layout(width='300px'), options=('KMeans', 'PKMâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "dropdown_algorithm = widgets.Dropdown(\n",
    "    options=['KMeans', 'PKMeans', 'Mux-Kmeans'],\n",
    "    value='KMeans',\n",
    "    description='Select Algorithm:',\n",
    "    style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'}\n",
    ")\n",
    "\n",
    "dropdown_dataset = widgets.Dropdown(\n",
    "    options=['KDD1999-10%', 'Syntetic200K-5', 'Syntetic200K-15'],\n",
    "    value='Syntetic200K-5',\n",
    "    description='Select Dataset:',\n",
    "    style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'}\n",
    ")\n",
    "\n",
    "# Sliders\n",
    "k_slider = widgets.IntSlider(value=5, min=5, max=15, step=1, description='K:', style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'})\n",
    "max_iter_slider = widgets.IntSlider(value=5, min=5, max=20, step=1, description='Max Iterations:',style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'})\n",
    "s_slider = widgets.IntSlider(value=6, min=4, max=10, step=1, description='S:', style = {'description_width':'initial'},\n",
    "    layout = {'width':'300px'})\n",
    "\n",
    "# Container to display sliders\n",
    "slider_container = widgets.VBox([k_slider, max_iter_slider])\n",
    "\n",
    "# Output area for results\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Update function to handle conditional slider\n",
    "def update_sliders(change):\n",
    "    if change['new'] == 'Mux-Kmeans':\n",
    "        slider_container.children = [k_slider, max_iter_slider, s_slider]\n",
    "    else:\n",
    "        slider_container.children = [k_slider, max_iter_slider]\n",
    "\n",
    "# Function to execute and display results\n",
    "def execute_action(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "\n",
    "        if dropdown_dataset.value == 'KDD1999-10%':\n",
    "            processed_data = processed_data_kdd\n",
    "            \n",
    "        elif dropdown_dataset.value == 'Syntetic200K-5':\n",
    "            processed_data = processed_data_5\n",
    "            \n",
    "        elif dropdown_dataset.value == 'Syntetic200K-15':\n",
    "            processed_data = processed_data_15\n",
    "        else:\n",
    "            print(\"Wrong Dataset\")\n",
    "            return\n",
    "\n",
    "        print(f\"{time.asctime()}\\tLoading the {dropdown_dataset.value} dataset...\")\n",
    "        np.savetxt(\"data.csv\", processed_data, delimiter=\",\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"KMeans\") \\\n",
    "            .master(\"spark://spark-master:7077\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        sc = spark.sparkContext\n",
    "        \n",
    "        rdd = sc.textFile(\"data.csv\")\n",
    "        \n",
    "        parsed_data = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "        parsed_data = parsed_data.cache()\n",
    "        \n",
    "        print(f\"{time.asctime()}\\tStarting {dropdown_algorithm.value} algorithm...\")\n",
    "        if dropdown_algorithm.value == \"KMeans\":\n",
    "            final_centorids, _ = simple_kmeans(processed_data, k=k_slider.value, max_iters=max_iter_slider.value)\n",
    "        elif dropdown_algorithm.value == \"PKMeans\":\n",
    "            final_centorids = pkmeans(sc, parsed_data, k=k_slider.value, max_iterations=max_iter_slider.value)\n",
    "        elif dropdown_algorithm.value == \"Mux-Kmeans\":\n",
    "            final_centorids = mux_kmeans(sc, parsed_data, k=k_slider.value, s=s_slider.value, max_iterations=max_iter_slider.value)\n",
    "        else:\n",
    "            print(\"Wrong Algorithm\")\n",
    "            return\n",
    "            \n",
    "        spark.stop()\n",
    "        \n",
    "        print(f\"{time.asctime()}\\tCalculating the silhouette score..\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"SilhouetteScore\") \\\n",
    "            .master(\"spark://spark-master:7077\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        labels = assign_clusters_to_data(processed_data, final_centorids)\n",
    "        spark_df = kmeans_to_spark_df(processed_data, labels, spark)\n",
    "        evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "        silhouette_score = evaluator.evaluate(spark_df)\n",
    "        \n",
    "        print(f\"{time.asctime()}\\tSilhouette Score: {silhouette_score}\")\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "# Observe dropdown changes\n",
    "dropdown_algorithm.observe(update_sliders, names='value')\n",
    "\n",
    "# Execute button\n",
    "execute_button = widgets.Button(description='Execute')\n",
    "execute_button.on_click(execute_action)\n",
    "display(widgets.VBox([dropdown_algorithm, dropdown_dataset, slider_container, execute_button, output_area]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
