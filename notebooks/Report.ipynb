{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae6aff-90a2-4e2d-818a-4ca905dc1dd1",
   "metadata": {},
   "source": [
    "### Download and prepare KDD1999 10% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2987b1-fdaa-4ca8-87f1-5c432c27d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def load_kddcup_data(filepath):\n",
    "    columns = [\n",
    "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "        'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "        'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "    ]\n",
    "    \n",
    "    data = pd.read_csv(filepath, header=None, names=columns)\n",
    "    return data\n",
    "\n",
    "def preprocess_kddcup_data(data):\n",
    "    # Separate features and labels\n",
    "    X = data.drop(columns=['label'])\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Convert all numerical features to standard normal destribution\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            # One hot encode categorical features\n",
    "            # ('cat', OneHotEncoder(), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor.fit_transform(X)\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7f4428-c740-46e3-a92d-f6165411d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "data_file_path = download_kddcup99(url, \"kdd_10_percent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f448e00f-eccf-474a-a627-d468e438d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Number of clusters\n",
    "max_iterations = 20\n",
    "num_partitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5150c65-d3bb-42d8-bb8c-84ad76a662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_kddcup_data(data_file_path)\n",
    "processed_data = preprocess_kddcup_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf0e40-fec5-4aec-9a1f-ba9a9ef920e1",
   "metadata": {},
   "source": [
    "### Simple KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f31326d9-2cfc-4cf3-b0d7-3d7fc0baf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_centroids(X, k, seed=314):\n",
    "    \"\"\"\n",
    "    Randomly initialize centroids from the dataset with a fixed seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random_indices = np.random.choice(X.shape[0], k, replace=False)\n",
    "    centroids = X[random_indices]\n",
    "    return centroids\n",
    "\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    Compute the distance between each data point and each centroid.\n",
    "    \"\"\"\n",
    "    distances = np.zeros((X.shape[0], len(centroids)))\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        distances[:, i] = np.linalg.norm(X - centroid, axis=1)\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"\n",
    "    Assign each data point to the closest centroid.\n",
    "    \"\"\"\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update centroids as the mean of all points assigned to each cluster.\n",
    "    \"\"\"\n",
    "    centroids = np.zeros((k, X.shape[1]))\n",
    "    for i in range(k):\n",
    "        points = X[labels == i]\n",
    "        if points.shape[0] > 0:\n",
    "            centroids[i] = points.mean(axis=0)\n",
    "    return centroids\n",
    "\n",
    "def simple_kmeans(X, k, max_iters=20, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Perform the K-Means clustering algorithm.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    for i in range(max_iters):\n",
    "        iteration_time = time.time()\n",
    "        old_centroids = centroids\n",
    "        distances = compute_distances(X, centroids)\n",
    "        labels = assign_clusters(distances)\n",
    "        centroids = update_centroids(X, labels, k)\n",
    "\n",
    "        print(f\"Iteration: {i}\\ttime taken: {time.time() - iteration_time}\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(np.abs(centroids - old_centroids) < tolerance):\n",
    "            print(f\"K-Means converged after {i+1} iterations.\")\n",
    "            break\n",
    "    print(f\"Total time: {time.time() - start_time}\")\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3e88f-2c13-400a-877e-c9dacf8b7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids, labels = simple_kmeans(processed_data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "505bfb8c-0fa4-4a44-bd29-5d44a18f9b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/14 14:50:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerClusterApp\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def kmeans_to_spark_df(X, labels):\n",
    "    \"\"\"\n",
    "    Convert NumPy array and cluster labels into a PySpark DataFrame with features and predictions.\n",
    "    \"\"\"\n",
    "    # Convert NumPy data to a list of Rows with features and predictions\n",
    "    rows = [Row(features=Vectors.dense(X[i]), prediction=int(labels[i])) for i in range(len(labels))]\n",
    "    return spark.createDataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "459c6d9c-65b4-4a75-812f-046b2d0fcee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m kmeans_to_spark_df(processed_data, \u001b[43mlabels\u001b[49m)\n\u001b[1;32m      3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m ClusteringEvaluator(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilhouette\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m silhouette_score \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(spark_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d01905-af96-4ea6-b356-dbfd20031044",
   "metadata": {},
   "source": [
    "# PKMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aed7f57-2365-476d-b194-77d883a88e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_data(X, centroids):\n",
    "    labels = []\n",
    "    for point in X:\n",
    "        distances = np.linalg.norm(centroids - point, axis=1)\n",
    "        labels.append(np.argmin(distances))\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e8b8b5-8deb-494d-9ee0-c25a786985d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import itertools\n",
    "import typing\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from itertools import groupby, compress\n",
    "\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PKmeans\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "def euclidean_dist(v1, v2):\n",
    "    return np.linalg.norm(v1-v2)\n",
    "    \n",
    "def closest_centroid(point, centroids):\n",
    "    # for dense data, we use euclidean distance\n",
    "    centroid_dist_pairs = map(lambda centroid: (centroid, euclidean_dist(point, centroid)), centroids)\n",
    "    return min(centroid_dist_pairs, key=lambda centroid_dist: centroid_dist[1])\n",
    "\n",
    "def clac_mean_points_in_cluster_pandas(distances):\n",
    "    # not in use. slower implementation of means calculation\n",
    "    df = pd.DataFrame(distances, columns=['Point', 'Centroid', 'Distance'])\n",
    "    df['Centroid'] = tuple(df['Centroid'])\n",
    "\n",
    "    return df.groupby('Centroid')['Point'].apply(np.mean).items()\n",
    "\n",
    "def clac_mean_points_in_cluster(distances):\n",
    "    def keyfunc(item):\n",
    "        return tuple(item[1])\n",
    "    \n",
    "    sorted_distances = sorted(distances, key=keyfunc)\n",
    "    for k, g in groupby(sorted_distances, keyfunc):\n",
    "        yield k, np.mean(array(list(g), dtype='object')[:,0], axis=0)\n",
    "\n",
    "def calc_partition_centroid_means(partition, centroids):\n",
    "    distances = map(lambda point: (point, \n",
    "                                   *closest_centroid(point, centroids)), \n",
    "                    partition)\n",
    "    \n",
    "    group = clac_mean_points_in_cluster(distances)\n",
    "    return group\n",
    "\n",
    "def closest_centroids_per_partition(data, centroids):\n",
    "    \"\"\"\n",
    "    calculate local means for each centroid on each partition to avoid transfer of large volume of data\n",
    "    \n",
    "    @param data: rdd, the actual data set\n",
    "    @param centroids:\n",
    "    @return: rdd of tuples (original centroid, mean of points close to this centroid on each partition)\n",
    "    \"\"\"\n",
    "    result = data.mapPartitions(lambda partition: calc_partition_centroid_means(partition, centroids.value))\n",
    "    return result\n",
    "\n",
    "def color_hash(point):\n",
    "    hsh = int(point[0] ** 2 + point[1] ** 2)\n",
    "    return '#' + str(hex(hsh % int('ffffff', 16)))[2:].zfill(6)\n",
    "\n",
    "def calc_error(new_centroids_series: pd.DataFrame):\n",
    "    old_and_new_centroids = array(list(new_centroids_series.items()))\n",
    "    error = np.linalg.norm((old_and_new_centroids[:,0] - old_and_new_centroids[:,1]))\n",
    "    return error\n",
    "\n",
    "def aggregate_means(closest_centroids):\n",
    "    df = pd.DataFrame(closest_centroids, columns=['Centroid', 'Point']) \n",
    "    new_centroids_series = df.groupby('Centroid')['Point'].apply(np.mean)\n",
    "    return new_centroids_series\n",
    "\n",
    "def pkmeans(data, n, max_iterations=15, stop_distance=0.01):\n",
    "    sampled_rdd = parsed_data.sample(withReplacement=False, fraction=0.001, seed=3144)\n",
    "    init_centroids = np.array(sampled_rdd.take(k))\n",
    "    centroids = sc.broadcast(init_centroids)\n",
    "    \n",
    "    iteration = 1\n",
    "    error = True\n",
    "    while error > stop_distance and iteration <= max_iterations:\n",
    "        loop_start = time.time()\n",
    "        closest_centroids_rdd = closest_centroids_per_partition(data, centroids)\n",
    "        closest_centroids = closest_centroids_rdd.collect()\n",
    "        return closest_centroids\n",
    "\n",
    "        new_centroids_series = aggregate_means(closest_centroids)\n",
    "        error = calc_error(new_centroids_series)\n",
    "        print(\"Iteration #{0}\\tDistance between old and new centroids: {1:.4f}\\tIteration took: {2:.4f} sec\".format(\n",
    "                                                                iteration, error, time.time() - loop_start))\n",
    "        centroids = sc.broadcast(new_centroids_series.to_list())\n",
    "        iteration += 1\n",
    "    \n",
    "    return new_centroids_series.to_list()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91ef754-dbfa-4ffe-82ec-103b0d6f0bb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mprocessed_data\u001b[49m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed_data' is not defined"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"data.csv\", processed_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065b785c-0c39-4c20-9fcf-7ec5244d2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data.csv\")\n",
    "\n",
    "parsed_data = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "parsed_data = parsed_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2228c870-281a-45b1-8e45-61bea6f9a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_centroids = pkmeans(parsed_data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a41c1-00a4-46b7-96a6-d95ba5184b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 14:51:29 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "24/12/14 14:51:29 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@398955bb[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@25fc6b13[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@163228ab]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@59435f0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/12/14 14:51:29 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@2f28bb12[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@28fe14b4[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@3f56ad7a]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@59435f0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/12/14 14:51:29 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@49300848[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@291d9230[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@232ad714]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@59435f0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/12/14 14:51:29 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@32a13a0d[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@e7debd2[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@7b9a1f7e]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@59435f0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/12/14 14:51:29 ERROR Utils: Uncaught exception in thread stop-spark-context\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:288)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:275)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:142)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:54)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2964)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2964)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2203)\n",
      "Caused by: org.apache.spark.SparkException: Could not find AppClient.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:72)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "final_centroids = pkmeans(parsed_data, k)\n",
    "labels = assign_clusters_to_data(processed_data, final_centroids)\n",
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
