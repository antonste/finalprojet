{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae6aff-90a2-4e2d-818a-4ca905dc1dd1",
   "metadata": {},
   "source": [
    "### Download and prepare KDD1999 10% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2987b1-fdaa-4ca8-87f1-5c432c27d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def load_kddcup_data(filepath):\n",
    "    columns = [\n",
    "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "        'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "        'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "    ]\n",
    "    \n",
    "    data = pd.read_csv(filepath, header=None, names=columns)\n",
    "    return data\n",
    "\n",
    "def preprocess_kddcup_data(data):\n",
    "    # Separate features and labels\n",
    "    X = data.drop(columns=['label'])\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Convert all numerical features to standard normal destribution\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            # One hot encode categorical features\n",
    "            # ('cat', OneHotEncoder(), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor.fit_transform(X)\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7f4428-c740-46e3-a92d-f6165411d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "data_file_path = download_kddcup99(url, \"kdd_10_percent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f448e00f-eccf-474a-a627-d468e438d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Number of clusters\n",
    "max_iterations = 20\n",
    "num_partitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5150c65-d3bb-42d8-bb8c-84ad76a662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_kddcup_data(data_file_path)\n",
    "processed_data = preprocess_kddcup_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf0e40-fec5-4aec-9a1f-ba9a9ef920e1",
   "metadata": {},
   "source": [
    "### Simple KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31326d9-2cfc-4cf3-b0d7-3d7fc0baf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_centroids(X, k, seed=314):\n",
    "    \"\"\"\n",
    "    Randomly initialize centroids from the dataset with a fixed seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random_indices = np.random.choice(X.shape[0], k, replace=False)\n",
    "    centroids = X[random_indices]\n",
    "    return centroids\n",
    "\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    Compute the distance between each data point and each centroid.\n",
    "    \"\"\"\n",
    "    distances = np.zeros((X.shape[0], len(centroids)))\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        distances[:, i] = np.linalg.norm(X - centroid, axis=1)\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"\n",
    "    Assign each data point to the closest centroid.\n",
    "    \"\"\"\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update centroids as the mean of all points assigned to each cluster.\n",
    "    \"\"\"\n",
    "    centroids = np.zeros((k, X.shape[1]))\n",
    "    for i in range(k):\n",
    "        points = X[labels == i]\n",
    "        if points.shape[0] > 0:\n",
    "            centroids[i] = points.mean(axis=0)\n",
    "    return centroids\n",
    "\n",
    "def simple_kmeans(X, k, max_iters=20, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Perform the K-Means clustering algorithm.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    for i in range(max_iters):\n",
    "        iteration_time = time.time()\n",
    "        old_centroids = centroids\n",
    "        distances = compute_distances(X, centroids)\n",
    "        labels = assign_clusters(distances)\n",
    "        centroids = update_centroids(X, labels, k)\n",
    "\n",
    "        print(f\"Iteration: {i}\\ttime taken: {time.time() - iteration_time}\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(np.abs(centroids - old_centroids) < tolerance):\n",
    "            print(f\"K-Means converged after {i+1} iterations.\")\n",
    "            break\n",
    "    print(f\"Total time: {time.time() - start_time}\")\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a3e88f-2c13-400a-877e-c9dacf8b7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\ttime taken: 1.1470954418182373\n",
      "Iteration: 1\ttime taken: 1.1386845111846924\n",
      "Iteration: 2\ttime taken: 1.1381573677062988\n",
      "Iteration: 3\ttime taken: 1.1244182586669922\n",
      "Iteration: 4\ttime taken: 1.131394386291504\n",
      "Iteration: 5\ttime taken: 1.1390714645385742\n",
      "Iteration: 6\ttime taken: 1.1399877071380615\n",
      "Iteration: 7\ttime taken: 1.1340887546539307\n",
      "Iteration: 8\ttime taken: 1.1325044631958008\n",
      "Iteration: 9\ttime taken: 1.1323182582855225\n",
      "Iteration: 10\ttime taken: 1.132812261581421\n",
      "Iteration: 11\ttime taken: 1.1314568519592285\n",
      "K-Means converged after 12 iterations.\n",
      "Total time: 13.63569450378418\n"
     ]
    }
   ],
   "source": [
    "centroids, labels = simple_kmeans(processed_data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "505bfb8c-0fa4-4a44-bd29-5d44a18f9b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/30 20:35:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\ttime taken: 1.2025256156921387\n",
      "Iteration: 1\ttime taken: 1.1561334133148193\n",
      "Iteration: 2\ttime taken: 1.148763656616211\n",
      "Iteration: 3\ttime taken: 1.1356165409088135\n",
      "Iteration: 4\ttime taken: 1.1407718658447266\n",
      "Iteration: 5\ttime taken: 1.1479098796844482\n",
      "Iteration: 6\ttime taken: 1.1337993144989014\n",
      "Iteration: 7\ttime taken: 1.1233820915222168\n",
      "Iteration: 8\ttime taken: 1.1338443756103516\n",
      "Iteration: 9\ttime taken: 1.1287212371826172\n",
      "Iteration: 10\ttime taken: 1.1295108795166016\n",
      "Iteration: 11\ttime taken: 1.1210384368896484\n",
      "K-Means converged after 12 iterations.\n",
      "Total time: 13.70983099937439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/30 20:36:02 WARN TaskSetManager: Stage 0 contains a task of very large size (17146 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/30 20:36:07 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "24/11/30 20:36:07 WARN TaskSetManager: Stage 1 contains a task of very large size (17146 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/30 20:36:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/11/30 20:36:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/11/30 20:36:09 WARN TaskSetManager: Stage 3 contains a task of very large size (17146 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.6279357756715807\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SilhouetteEvaluation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def kmeans_to_spark_df(X, labels):\n",
    "    \"\"\"\n",
    "    Convert NumPy array and cluster labels into a PySpark DataFrame with features and predictions.\n",
    "    \"\"\"\n",
    "    # Convert NumPy data to a list of Rows with features and predictions\n",
    "    rows = [Row(features=Vectors.dense(X[i]), prediction=int(labels[i])) for i in range(len(labels))]\n",
    "    return spark.createDataFrame(rows)\n",
    "\n",
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ac001-144e-4693-bd6d-4c0119b6451f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
