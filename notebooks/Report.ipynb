{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae6aff-90a2-4e2d-818a-4ca905dc1dd1",
   "metadata": {},
   "source": [
    "### Download and prepare KDD1999 10% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2987b1-fdaa-4ca8-87f1-5c432c27d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def load_kddcup_data(filepath):\n",
    "    columns = [\n",
    "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "        'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "        'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "    ]\n",
    "    \n",
    "    data = pd.read_csv(filepath, header=None, names=columns)\n",
    "    return data\n",
    "\n",
    "def preprocess_kddcup_data(data):\n",
    "    # Separate features and labels\n",
    "    X = data.drop(columns=['label'])\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Convert all numerical features to standard normal destribution\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            # One hot encode categorical features\n",
    "            # ('cat', OneHotEncoder(), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor.fit_transform(X)\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7f4428-c740-46e3-a92d-f6165411d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "data_file_path = download_kddcup99(url, \"kdd_10_percent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f448e00f-eccf-474a-a627-d468e438d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Number of clusters\n",
    "max_iterations = 20\n",
    "num_partitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5150c65-d3bb-42d8-bb8c-84ad76a662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_kddcup_data(data_file_path)\n",
    "processed_data = preprocess_kddcup_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf0e40-fec5-4aec-9a1f-ba9a9ef920e1",
   "metadata": {},
   "source": [
    "### Simple KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f31326d9-2cfc-4cf3-b0d7-3d7fc0baf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "def initialize_centroids(X, k, seed=314):\n",
    "    \"\"\"\n",
    "    Randomly initialize centroids from the dataset with a fixed seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_indices = random.sample(range(len(X)), k)\n",
    "    centroids = [X[idx][:] for idx in random_indices]  # Copy selected points\n",
    "    return centroids\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"Compute the Euclidean distance between two points.\"\"\"\n",
    "    return math.sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(point1, point2)))\n",
    "\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    Compute the distance between each data point and each centroid.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for point in X:\n",
    "        distances.append([euclidean_distance(point, centroid) for centroid in centroids])\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"\n",
    "    Assign each data point to the closest centroid.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for distance_list in distances:\n",
    "        min_distance_index = distance_list.index(min(distance_list))\n",
    "        labels.append(min_distance_index)\n",
    "    return labels\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update centroids as the mean of all points assigned to each cluster.\n",
    "    \"\"\"\n",
    "    centroids = [[0] * len(X[0]) for _ in range(k)]\n",
    "    counts = [0] * k\n",
    "    \n",
    "    # Sum points for each cluster\n",
    "    for idx, label in enumerate(labels):\n",
    "        for dim in range(len(X[idx])):\n",
    "            centroids[label][dim] += X[idx][dim]\n",
    "        counts[label] += 1\n",
    "    \n",
    "    # Divide by the count to compute the mean\n",
    "    for i in range(k):\n",
    "        if counts[i] > 0:  # Avoid division by zero\n",
    "            centroids[i] = [val / counts[i] for val in centroids[i]]\n",
    "    return centroids\n",
    "\n",
    "def has_converged(new_centroids, old_centroids, tolerance):\n",
    "    \"\"\"\n",
    "    Check if centroids have converged based on a given tolerance.\n",
    "    \"\"\"\n",
    "    for nc, oc in zip(new_centroids, old_centroids):\n",
    "        if any(abs(n - o) > tolerance for n, o in zip(nc, oc)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def simple_kmeans(X, k, max_iters=150, tolerance=0):\n",
    "    \"\"\"\n",
    "    Perform the K-Means clustering algorithm.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        iteration_time = time.time()\n",
    "        old_centroids = [c[:] for c in centroids]  # Deep copy\n",
    "        distances = compute_distances(X, centroids)\n",
    "        labels = assign_clusters(distances)\n",
    "        centroids = update_centroids(X, labels, k)\n",
    "        \n",
    "        print(f\"Iteration: {i}\\ttime taken: {time.time() - iteration_time:.4f} seconds\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if has_converged(centroids, old_centroids, tolerance):\n",
    "            print(f\"K-Means converged after {i+1} iterations.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total time: {time.time() - start_time:.4f} seconds\")\n",
    "    return centroids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a3e88f-2c13-400a-877e-c9dacf8b7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\ttime taken: 19.4609 seconds\n",
      "Iteration: 1\ttime taken: 18.6644 seconds\n",
      "Iteration: 2\ttime taken: 18.4806 seconds\n",
      "Iteration: 3\ttime taken: 18.4261 seconds\n",
      "Iteration: 4\ttime taken: 18.2458 seconds\n",
      "Iteration: 5\ttime taken: 18.3331 seconds\n",
      "Iteration: 6\ttime taken: 18.4883 seconds\n",
      "Iteration: 7\ttime taken: 18.1828 seconds\n",
      "Iteration: 8\ttime taken: 18.4302 seconds\n",
      "Iteration: 9\ttime taken: 18.2313 seconds\n",
      "Iteration: 10\ttime taken: 18.1955 seconds\n",
      "Iteration: 11\ttime taken: 18.4371 seconds\n",
      "Iteration: 12\ttime taken: 18.2244 seconds\n",
      "Iteration: 13\ttime taken: 18.4230 seconds\n",
      "K-Means converged after 14 iterations.\n",
      "Total time: 258.2253 seconds\n"
     ]
    }
   ],
   "source": [
    "centroids, labels = simple_kmeans(processed_data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505bfb8c-0fa4-4a44-bd29-5d44a18f9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14c71c-8999-4cad-961c-85275902fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerClusterApp\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02812042-45b6-477f-8609-0f062735a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_to_spark_df(X, labels):\n",
    "    \"\"\"\n",
    "    Convert NumPy array and cluster labels into a PySpark DataFrame with features and predictions.\n",
    "    \"\"\"\n",
    "    # Convert NumPy data to a list of Rows with features and predictions\n",
    "    rows = [Row(features=Vectors.dense(X[i]), prediction=int(labels[i])) for i in range(len(labels))]\n",
    "    return spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459c6d9c-65b4-4a75-812f-046b2d0fcee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/17 18:50:07 WARN TaskSetManager: Stage 0 contains a task of very large size (18932 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/17 18:50:13 WARN TaskSetManager: Stage 1 contains a task of very large size (18932 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/17 18:50:17 WARN TaskSetManager: Stage 3 contains a task of very large size (18932 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.6579106198643335\n"
     ]
    }
   ],
   "source": [
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d01905-af96-4ea6-b356-dbfd20031044",
   "metadata": {},
   "source": [
    "# PKMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aed7f57-2365-476d-b194-77d883a88e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_data(X, centroids):\n",
    "    labels = []\n",
    "    for point in X:\n",
    "        distances = np.linalg.norm(centroids - point, axis=1)\n",
    "        labels.append(np.argmin(distances))\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e8b8b5-8deb-494d-9ee0-c25a786985d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import itertools\n",
    "import typing\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from itertools import groupby, compress\n",
    "\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "def euclidean_dist(v1, v2):\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2, axis=1))\n",
    "\n",
    "def closest_centroid(points, centroids):\n",
    "    distances = np.sqrt(((points[:, None] - centroids[None, :]) ** 2).sum(axis=2))\n",
    "    closest_centroids_indices = np.argmin(distances, axis=1)\n",
    "    return closest_centroids_indices\n",
    "\n",
    "def calc_partition_centroid_means(partition, centroids):\n",
    "    points = np.array(list(partition)) \n",
    "    if len(points) == 0:  # Handle empty partitions\n",
    "        return []\n",
    "\n",
    "    closest_indices = closest_centroid(points, centroids.value)\n",
    "    \n",
    "    # Combine points with their respective closest centroids\n",
    "    data = pd.DataFrame({\n",
    "        \"Centroid\": closest_indices,\n",
    "        \"Point\": list(points)\n",
    "    })\n",
    "\n",
    "    # Expand Point into multiple dimensions\n",
    "    point_df = pd.DataFrame(data['Point'].tolist(), index=data.index)\n",
    "    combined_df = pd.concat([data['Centroid'], point_df], axis=1)\n",
    "\n",
    "    # Group by Centroid and calculate mean for each group\n",
    "    means = combined_df.groupby('Centroid').mean().reset_index().values\n",
    "    return means\n",
    "\n",
    "def closest_centroids(data, centroids):\n",
    "    return data.mapPartitions(lambda partition: [calc_partition_centroid_means(partition, centroids)])\n",
    "\n",
    "def aggregate_means(rdd):\n",
    "    partition_means = np.concatenate(rdd.collect(), axis=0)\n",
    "\n",
    "    num_dimensions = partition_means.shape[1] - 1  # Subtract 1 for 'Centroid'\n",
    "    columns = ['Centroid'] + [f'dim_{i}' for i in range(num_dimensions)]\n",
    "\n",
    "    df = pd.DataFrame(partition_means, columns=columns)\n",
    "\n",
    "    grouped_means = df.groupby('Centroid').mean()\n",
    "    return grouped_means\n",
    "\n",
    "def handle_missing_centorids(aggregated_centroids, old_centroids):\n",
    "    num_centorids = len(old_centroids)\n",
    "    missing_centroids = set(range(num_centorids)) - set(aggregated_centroids.index)\n",
    "    for ix in missing_centroids:\n",
    "        # Copy old centroids to replace the missing\n",
    "        aggregated_centroids.loc[ix] = old_centroids[ix]\n",
    "    return aggregated_centroids.sort_index().to_numpy()\n",
    "\n",
    "def calc_error(new_centroids, old_centroids):\n",
    "    return np.sum(euclidean_dist(new_centroids, old_centroids))\n",
    "\n",
    "def pkmeans(data, n, max_iterations=150, stop_distance=0.001):\n",
    "    print(time.asctime(), \"Started\")\n",
    "    start_time = time.time()\n",
    "    init_centroids = np.array(data.takeSample(False, n, seed=42)) \n",
    "    centroids = sc.broadcast(init_centroids)\n",
    "\n",
    "    iteration = 1\n",
    "    error = float(\"inf\")\n",
    "    while error > stop_distance and iteration <= max_iterations:\n",
    "        loop_start = time.time()\n",
    "        closest_centroids_rdd = closest_centroids(data, centroids)\n",
    "        aggregated_centroids = aggregate_means(closest_centroids_rdd)\n",
    "        new_centroids = handle_missing_centorids(aggregated_centroids, centroids.value)\n",
    "        error = calc_error(new_centroids, centroids.value)\n",
    "        print(\"{3} Iteration #{0}\\tDistance between old and new centroids: {1:.4f}\\tIteration took: {2:.4f} sec\".format(\n",
    "            iteration, error, time.time() - loop_start, time.asctime()))\n",
    "        \n",
    "        # Update centroids\n",
    "        centroids = sc.broadcast(new_centroids) \n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"Total time: {time.time() - start_time}\")\n",
    "\n",
    "    return centroids.value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91ef754-dbfa-4ffe-82ec-103b0d6f0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data.csv\", processed_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "065b785c-0c39-4c20-9fcf-7ec5244d2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/17 19:56:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PKmeans\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile(\"data.csv\")\n",
    "\n",
    "parsed_data = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "parsed_data = parsed_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2228c870-281a-45b1-8e45-61bea6f9a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:02 2024 Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:07 2024 Iteration #1\tDistance between old and new centroids: 11.1520\tIteration took: 2.0140 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:09 2024 Iteration #2\tDistance between old and new centroids: 13.1178\tIteration took: 1.6981 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:11 2024 Iteration #3\tDistance between old and new centroids: 13.4210\tIteration took: 1.6940 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:12 2024 Iteration #4\tDistance between old and new centroids: 19.5009\tIteration took: 1.7703 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:14 2024 Iteration #5\tDistance between old and new centroids: 31.9940\tIteration took: 1.7832 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:16 2024 Iteration #6\tDistance between old and new centroids: 38.0124\tIteration took: 1.6682 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:18 2024 Iteration #7\tDistance between old and new centroids: 13.4152\tIteration took: 1.6994 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:19 2024 Iteration #8\tDistance between old and new centroids: 5.1324\tIteration took: 1.7395 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:21 2024 Iteration #9\tDistance between old and new centroids: 2.2159\tIteration took: 1.6679 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:23 2024 Iteration #10\tDistance between old and new centroids: 0.3901\tIteration took: 1.7016 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:24 2024 Iteration #11\tDistance between old and new centroids: 0.4805\tIteration took: 1.6348 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:26 2024 Iteration #12\tDistance between old and new centroids: 0.9735\tIteration took: 1.5997 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:28 2024 Iteration #13\tDistance between old and new centroids: 1.0434\tIteration took: 1.6163 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:29 2024 Iteration #14\tDistance between old and new centroids: 0.5726\tIteration took: 1.6341 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:31 2024 Iteration #15\tDistance between old and new centroids: 0.2786\tIteration took: 1.6155 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:32 2024 Iteration #16\tDistance between old and new centroids: 0.1388\tIteration took: 1.5776 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:34 2024 Iteration #17\tDistance between old and new centroids: 0.0383\tIteration took: 1.5980 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:36 2024 Iteration #18\tDistance between old and new centroids: 0.0235\tIteration took: 1.9376 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:38 2024 Iteration #19\tDistance between old and new centroids: 0.0025\tIteration took: 1.7273 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:39 2024 Iteration #20\tDistance between old and new centroids: 0.0020\tIteration took: 1.6290 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=====================================>                  (10 + 5) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 10:47:41 2024 Iteration #21\tDistance between old and new centroids: 0.0000\tIteration took: 1.9044 sec\n",
      "Total time: 39.3495032787323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_centorids = pkmeans(parsed_data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "148a41c1-00a4-46b7-96a6-d95ba5184b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 10:47:55 WARN TaskSetManager: Stage 23 contains a task of very large size (18932 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/16 10:47:59 WARN TaskSetManager: Stage 24 contains a task of very large size (18932 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/16 10:48:02 WARN TaskSetManager: Stage 26 contains a task of very large size (18932 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 26:===================>                                      (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.615598344068801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labels = assign_clusters_to_data(processed_data, final_centorids)\n",
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f0fd64-2b72-4df6-8c70-33d5386a7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity score between two centroid groups.\n",
    "    The similarity is the inverse of the sum of distances between corresponding centroids.\n",
    "    \"\"\"\n",
    "    distances = [np.linalg.norm(c1 - c2) for c1, c2 in zip(group1, group2)]\n",
    "    return 1 / sum(distances) if sum(distances) != 0 else float('inf')\n",
    "\n",
    "\n",
    "def find_most_dissimilar_group(target_group, groups):\n",
    "    \"\"\"\n",
    "    Find the most dissimilar group to the target group.\n",
    "    \"\"\"\n",
    "    max_dissimilarity = -float('inf')\n",
    "    most_dissimilar = None\n",
    "    \n",
    "    for group in groups:\n",
    "        if np.array_equal(target_group, group):\n",
    "            continue  # Skip self-comparison\n",
    "        \n",
    "        similarity = calculate_similarity(target_group, group)\n",
    "        if similarity > max_dissimilarity:\n",
    "            max_dissimilarity = similarity\n",
    "            most_dissimilar = group\n",
    "    \n",
    "    return most_dissimilar\n",
    "\n",
    "\n",
    "def adgp(groups):\n",
    "    \"\"\"\n",
    "    Generate new centroid groups using the Average of Dissimilar Group Pairs (ADGP).\n",
    "    \"\"\"\n",
    "    new_groups = []\n",
    "    group_count = len(groups)\n",
    "    \n",
    "    for i, group1 in enumerate(groups):\n",
    "        group2 = find_most_dissimilar_group(group1, groups)\n",
    "        \n",
    "        # Compute the average of corresponding centroids to form a new group\n",
    "        new_group = [(c1 + c2) / 2 for c1, c2 in zip(group1, group2)]\n",
    "        new_groups.append(new_group)\n",
    "    \n",
    "    return new_groups\n",
    "\n",
    "# Permutation function to align centroids across groups\n",
    "def permute_centroids(centroid_groups):\n",
    "    base_group = centroid_groups[0]\n",
    "    permuted_groups = []\n",
    "\n",
    "    for group in centroid_groups[1:]:\n",
    "        # Track matched indices to prevent duplication\n",
    "        matched = set()\n",
    "        permuted_group = []\n",
    "        for base_c in base_group:\n",
    "            # Find the closest unmatched centroid\n",
    "            distances = [(i, np.linalg.norm(base_c - c)) for i, c in enumerate(group) if i not in matched]\n",
    "            if distances:\n",
    "                closest_idx = min(distances, key=lambda x: x[1])[0]\n",
    "                permuted_group.append(group[closest_idx])\n",
    "                matched.add(closest_idx)\n",
    "            else:\n",
    "                # Handle unmatched cases by using a default\n",
    "                permuted_group.append(base_c)\n",
    "        permuted_groups.append(permuted_group)\n",
    "    return [base_group] + permuted_groups\n",
    "\n",
    "def initialize_centroid_groups(parsed_data, k, s):\n",
    "    # Collect a small subset of the data for initialization\n",
    "    sample_data = parsed_data.takeSample(False, k * s, seed=1)\n",
    "    centroid_groups = [\n",
    "        sample_data[i * k:(i + 1) * k] for i in range(s)\n",
    "    ]\n",
    "    return np.array(centroid_groups)\n",
    "\n",
    "def mux_kmeans(data, k, s, max_iterations=20):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize centroid groups\n",
    "    initial_centroids_groups = initialize_centroid_groups(data, k, s)\n",
    "    \n",
    "    # Mux-Kmeans main loop\n",
    "    for iteration in range(max_iterations):\n",
    "        iteration_time = time.time()\n",
    "        twcv_scores = []\n",
    "        \n",
    "        # Evaluate centroid groups\n",
    "        for centroids in initial_centroids_groups:\n",
    "            centroids_broadcast = sc.broadcast(centroids)\n",
    "    \n",
    "            # Assign points to clusters\n",
    "            clustered_rdd = data.map(\n",
    "                lambda p: (\n",
    "                    np.argmin([np.linalg.norm(np.subtract(p, c)) for c in centroids_broadcast.value]),\n",
    "                    (p, 1)\n",
    "                )\n",
    "            ) # (Cluster index, (point, 1))\n",
    "\n",
    "            # Recalculate centroids\n",
    "            new_centroids = (\n",
    "                clustered_rdd\n",
    "                .reduceByKey(lambda x, y: (np.add(x[0], y[0]), x[1] + y[1]))  # Sum points and count\n",
    "                .map(lambda x: (x[0], x[1][0] / x[1][1]))  # Compute new centroids\n",
    "                .collectAsMap()\n",
    "            )\n",
    "\n",
    "            new_centroids_arr = np.array(\n",
    "                    [new_centroids[j] if j in new_centroids else centroids[j] for j in range(len(centroids))]\n",
    "            )\n",
    "    \n",
    "            # Update the centroids to the new centroids\n",
    "            centroids_broadcast = sc.broadcast(new_centroids_arr)\n",
    "            \n",
    "            # Assign points to the nearest new centroids\n",
    "            centroid_to_point = data.map(\n",
    "                lambda point: (\n",
    "                    np.argmin([np.linalg.norm(point - c) for c in centroids_broadcast.value]), # cluster_id\n",
    "                    point # original point\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            # Calculate TWCV for the current group\n",
    "            # Row is (cluster_id, point)\n",
    "            twcv = centroid_to_point.map(lambda row: np.linalg.norm(row[1] - centroids_broadcast.value[row[0]]) ** 2).sum()\n",
    "            twcv_scores.append((new_centroids_arr, twcv))\n",
    "    \n",
    "        # Prune and incubate\n",
    "        twcv_scores.sort(key=lambda x: x[1])\n",
    "        best_groups = twcv_scores[:s // 2]\n",
    "        best_centroids = [x[0] for x in best_groups]\n",
    "    \n",
    "        # Permutation\n",
    "        aligned_centroid_groups = permute_centroids(best_centroids)\n",
    "        \n",
    "        # Incubate new groups\n",
    "        new_centroid_groups = []\n",
    "        for group in aligned_centroid_groups:\n",
    "            new_centroid_groups.append(adgp(group))\n",
    "    \n",
    "        # Prepare for the next iteration\n",
    "        initial_centroids_groups = best_centroids + new_centroid_groups\n",
    "        \n",
    "        best_twcv = min(twcv_scores, key=lambda x: x[1])[1]\n",
    "\n",
    "        print(f\"Iteration: {iteration}\\ttime taken: {time.time() - iteration_time:.4f} seconds\\tBest TWCV: {best_twcv}\")\n",
    "\n",
    "    # Select the final best group\n",
    "    final_group = min(twcv_scores, key=lambda x: x[1])\n",
    "    print(f\"Total time: {time.time() - start_time:.4f} seconds\")\n",
    "    return final_group[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac5187e-16f4-45d1-a1cb-78b79cc4e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/21 14:36:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\ttime taken: 23.8191 seconds\tBest TWCV: 10361162.584906505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\ttime taken: 23.7854 seconds\tBest TWCV: 9891506.035477528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2\ttime taken: 23.4082 seconds\tBest TWCV: 9847454.353318198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3\ttime taken: 23.9537 seconds\tBest TWCV: 9846769.03706399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4\ttime taken: 23.7534 seconds\tBest TWCV: 9846736.705185255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5\ttime taken: 22.6463 seconds\tBest TWCV: 9846731.241614208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6\ttime taken: 22.6670 seconds\tBest TWCV: 9846731.061911369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7\ttime taken: 25.4484 seconds\tBest TWCV: 9846731.03228543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8\ttime taken: 23.3847 seconds\tBest TWCV: 9846731.03228543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9\ttime taken: 24.2897 seconds\tBest TWCV: 9846731.03228543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\ttime taken: 22.5967 seconds\tBest TWCV: 9846731.03228543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11\ttime taken: 22.5518 seconds\tBest TWCV: 9846731.03228543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12\ttime taken: 23.1161 seconds\tBest TWCV: 9835561.933573037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13\ttime taken: 22.6853 seconds\tBest TWCV: 9830718.41686887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14\ttime taken: 22.6829 seconds\tBest TWCV: 9827567.668052265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15\ttime taken: 22.9280 seconds\tBest TWCV: 9827407.692029335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16\ttime taken: 24.5223 seconds\tBest TWCV: 9827406.47899581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17\ttime taken: 22.8960 seconds\tBest TWCV: 9827406.469603414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18\ttime taken: 22.5367 seconds\tBest TWCV: 9827406.456729176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/21 14:44:08 WARN Dispatcher: Message RemoteProcessDisconnected(172.18.0.5:45342) dropped. Could not find OutputCommitCoordinator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19\ttime taken: 22.4149 seconds\tBest TWCV: 9827406.451064702\n",
      "Total time: 470.5373 seconds\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mux-KMeans\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile(\"data.csv\")\n",
    "\n",
    "parsed_data = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "parsed_data = parsed_data.cache()\n",
    "final_centorids = mux_kmeans(parsed_data, k, s=6)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b85db287-9815-409a-9ae1-f3dcea27fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/21 14:45:49 WARN TaskSetManager: Stage 0 contains a task of very large size (21431 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/21 14:45:54 WARN TaskSetManager: Stage 1 contains a task of very large size (21431 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/21 14:45:58 WARN TaskSetManager: Stage 3 contains a task of very large size (21431 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.657910619864277\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mux-KMeans-Silhouette\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "labels = assign_clusters_to_data(processed_data, final_centorids)\n",
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
