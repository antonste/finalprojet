{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae6aff-90a2-4e2d-818a-4ca905dc1dd1",
   "metadata": {},
   "source": [
    "### Download and prepare KDD1999 10% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "fb2987b1-fdaa-4ca8-87f1-5c432c27d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def load_kddcup_data(filepath):\n",
    "    columns = [\n",
    "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "        'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "        'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "    ]\n",
    "    \n",
    "    data = pd.read_csv(filepath, header=None, names=columns)\n",
    "    return data\n",
    "\n",
    "def preprocess_kddcup_data(data):\n",
    "    # Separate features and labels\n",
    "    X = data.drop(columns=['label'])\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Convert all numerical features to standard normal destribution\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            # One hot encode categorical features\n",
    "            # ('cat', OneHotEncoder(), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor.fit_transform(X)\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file\n",
    "\n",
    "def download_kddcup99(url, destination_file_name, destination_folder=\"/home/jovyan/work/data\"):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    compressed_file = os.path.join(destination_folder, destination_file_name + \".gz\")\n",
    "    extracted_file = os.path.join(destination_folder, destination_file_name)\n",
    "    \n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(compressed_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urlretrieve(url, compressed_file)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(\"Extracting dataset...\")\n",
    "        os.system(f\"gunzip {compressed_file}\")\n",
    "        print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1b7f4428-c740-46e3-a92d-f6165411d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "data_file_path = download_kddcup99(url, \"kdd_10_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f448e00f-eccf-474a-a627-d468e438d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Number of clusters\n",
    "max_iterations = 20\n",
    "num_partitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d5150c65-d3bb-42d8-bb8c-84ad76a662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_kddcup_data(data_file_path)\n",
    "processed_data = preprocess_kddcup_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d7a9b854-de85-410d-a438-ba350dff34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "processed_data, y = make_blobs(n_samples=200000, n_features=25, centers=k, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf0e40-fec5-4aec-9a1f-ba9a9ef920e1",
   "metadata": {},
   "source": [
    "### Simple KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f31326d9-2cfc-4cf3-b0d7-3d7fc0baf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "def initialize_centroids(X, k, seed=314):\n",
    "    \"\"\"\n",
    "    Randomly initialize centroids from the dataset with a fixed seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_indices = random.sample(range(len(X)), k)\n",
    "    centroids = [X[idx][:] for idx in random_indices]  # Copy selected points\n",
    "    return centroids\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"Compute the Euclidean distance between two points.\"\"\"\n",
    "    return math.sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(point1, point2)))\n",
    "\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    Compute the distance between each data point and each centroid.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for point in X:\n",
    "        distances.append([euclidean_distance(point, centroid) for centroid in centroids])\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"\n",
    "    Assign each data point to the closest centroid.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for distance_list in distances:\n",
    "        min_distance_index = distance_list.index(min(distance_list))\n",
    "        labels.append(min_distance_index)\n",
    "    return labels\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update centroids as the mean of all points assigned to each cluster.\n",
    "    \"\"\"\n",
    "    centroids = [[0] * len(X[0]) for _ in range(k)]\n",
    "    counts = [0] * k\n",
    "    \n",
    "    # Sum points for each cluster\n",
    "    for idx, label in enumerate(labels):\n",
    "        for dim in range(len(X[idx])):\n",
    "            centroids[label][dim] += X[idx][dim]\n",
    "        counts[label] += 1\n",
    "    \n",
    "    # Divide by the count to compute the mean\n",
    "    for i in range(k):\n",
    "        if counts[i] > 0:  # Avoid division by zero\n",
    "            centroids[i] = [val / counts[i] for val in centroids[i]]\n",
    "    return centroids\n",
    "\n",
    "def has_converged(new_centroids, old_centroids, tolerance):\n",
    "    \"\"\"\n",
    "    Check if centroids have converged based on a given tolerance.\n",
    "    \"\"\"\n",
    "    for nc, oc in zip(new_centroids, old_centroids):\n",
    "        if any(abs(n - o) > tolerance for n, o in zip(nc, oc)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def simple_kmeans(X, k, max_iters=150, tolerance=0):\n",
    "    \"\"\"\n",
    "    Perform the K-Means clustering algorithm.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        iteration_time = time.time()\n",
    "        old_centroids = [c[:] for c in centroids]  # Deep copy\n",
    "        distances = compute_distances(X, centroids)\n",
    "        labels = assign_clusters(distances)\n",
    "        centroids = update_centroids(X, labels, k)\n",
    "        \n",
    "        print(f\"Iteration: {i}\\ttime taken: {time.time() - iteration_time:.4f} seconds\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if has_converged(centroids, old_centroids, tolerance):\n",
    "            print(f\"K-Means converged after {i+1} iterations.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total time: {time.time() - start_time:.4f} seconds\")\n",
    "    return centroids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "28a3e88f-2c13-400a-877e-c9dacf8b7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\ttime taken: 5.3990 seconds\n",
      "Iteration: 1\ttime taken: 4.4837 seconds\n",
      "Iteration: 2\ttime taken: 4.5242 seconds\n",
      "Iteration: 3\ttime taken: 4.6208 seconds\n",
      "Iteration: 4\ttime taken: 5.1374 seconds\n",
      "Iteration: 5\ttime taken: 4.9583 seconds\n",
      "Iteration: 6\ttime taken: 4.8987 seconds\n",
      "Iteration: 7\ttime taken: 4.4362 seconds\n",
      "Iteration: 8\ttime taken: 4.6242 seconds\n",
      "Iteration: 9\ttime taken: 4.7706 seconds\n",
      "Iteration: 10\ttime taken: 4.4431 seconds\n",
      "Iteration: 11\ttime taken: 4.6651 seconds\n",
      "Iteration: 12\ttime taken: 4.7137 seconds\n",
      "Iteration: 13\ttime taken: 4.4435 seconds\n",
      "Iteration: 14\ttime taken: 4.7334 seconds\n",
      "Iteration: 15\ttime taken: 4.7438 seconds\n",
      "Iteration: 16\ttime taken: 4.4454 seconds\n",
      "Iteration: 17\ttime taken: 4.5979 seconds\n",
      "Iteration: 18\ttime taken: 4.7834 seconds\n",
      "Iteration: 19\ttime taken: 4.4805 seconds\n",
      "Total time: 93.9052 seconds\n"
     ]
    }
   ],
   "source": [
    "centroids, labels = simple_kmeans(processed_data, k, max_iters=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "505bfb8c-0fa4-4a44-bd29-5d44a18f9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ac14c71c-8999-4cad-961c-85275902fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 18:17:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerClusterApp\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "02812042-45b6-477f-8609-0f062735a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_to_spark_df(X, labels):\n",
    "    \"\"\"\n",
    "    Convert NumPy array and cluster labels into a PySpark DataFrame with features and predictions.\n",
    "    \"\"\"\n",
    "    # Convert NumPy data to a list of Rows with features and predictions\n",
    "    rows = [Row(features=Vectors.dense(X[i]), prediction=int(labels[i])) for i in range(len(labels))]\n",
    "    return spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "459c6d9c-65b4-4a75-812f-046b2d0fcee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 18:17:25 WARN TaskSetManager: Stage 253 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/02 18:17:26 WARN TaskSetManager: Stage 254 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/02 18:17:26 WARN TaskSetManager: Stage 256 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.5907309307628911\n"
     ]
    }
   ],
   "source": [
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d01905-af96-4ea6-b356-dbfd20031044",
   "metadata": {},
   "source": [
    "# PKMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9aed7f57-2365-476d-b194-77d883a88e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_data(X, centroids):\n",
    "    labels = []\n",
    "    for point in X:\n",
    "        distances = np.linalg.norm(centroids - point, axis=1)\n",
    "        labels.append(np.argmin(distances))\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "42e8b8b5-8deb-494d-9ee0-c25a786985d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import itertools\n",
    "import typing\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from itertools import groupby, compress\n",
    "\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "def euclidean_dist(v1, v2):\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2, axis=1))\n",
    "\n",
    "def closest_centroid(points, centroids):\n",
    "    distances = np.sqrt(((points[:, None] - centroids[None, :]) ** 2).sum(axis=2))\n",
    "    closest_centroids_indices = np.argmin(distances, axis=1)\n",
    "    return closest_centroids_indices\n",
    "\n",
    "def calc_partition_centroid_means(partition, centroids):\n",
    "    points = np.array(list(partition)) \n",
    "    if len(points) == 0:  # Handle empty partitions\n",
    "        return []\n",
    "\n",
    "    closest_indices = closest_centroid(points, centroids.value)\n",
    "    \n",
    "    # Combine points with their respective closest centroids\n",
    "    data = pd.DataFrame({\n",
    "        \"Centroid\": closest_indices,\n",
    "        \"Point\": list(points)\n",
    "    })\n",
    "\n",
    "    # Expand Point into multiple dimensions\n",
    "    point_df = pd.DataFrame(data['Point'].tolist(), index=data.index)\n",
    "    combined_df = pd.concat([data['Centroid'], point_df], axis=1)\n",
    "\n",
    "    # Group by Centroid and calculate sum and count for each group\n",
    "    grouped = combined_df.groupby('Centroid').agg(['sum', 'count'])\n",
    "\n",
    "    grouped.columns = ['_'.join(map(str, col)).strip() for col in grouped.columns.to_flat_index()]\n",
    "    grouped.reset_index(inplace=True)\n",
    "\n",
    "    # Extract sum and count separately\n",
    "    point_sums = grouped[[f'{i}_sum' for i in range(points.shape[1])]].to_numpy()\n",
    "    counts = grouped['0_count'].to_numpy()\n",
    "\n",
    "    # Return 2 array with each row: [Centroid, *points sums, count]\n",
    "    return np.column_stack([grouped['Centroid'].to_numpy(), point_sums, counts])\n",
    "\n",
    "def aggregate_means(rdd):\n",
    "    partition_means = np.concatenate(rdd.collect(), axis=0)\n",
    "\n",
    "    # Extract dimensions and count\n",
    "    num_dimensions = int(partition_means.shape[1] - 2)  # Exclude centroid and count\n",
    "    sums = partition_means[:, 1:1 + num_dimensions]\n",
    "    counts = partition_means[:, -1]\n",
    "\n",
    "    # Aggregate sums and counts by centroid\n",
    "    data = pd.DataFrame(partition_means, columns=['Centroid'] + [f'dim_{i}_sum' for i in range(num_dimensions)] + ['count'])\n",
    "    aggregated = data.groupby('Centroid').sum()\n",
    "    aggregated_sums = aggregated[[f'dim_{i}_sum' for i in range(num_dimensions)]].to_numpy()\n",
    "    aggregated_counts = aggregated['count'].to_numpy()\n",
    "\n",
    "    # Compute weighted averages\n",
    "    new_centroids = aggregated_sums / aggregated_counts[:, None]\n",
    "\n",
    "    return pd.DataFrame(new_centroids, index=aggregated.index)\n",
    "\n",
    "def handle_missing_centroids(aggregated_centroids, old_centroids):\n",
    "    num_centroids = len(old_centroids)\n",
    "    missing_centroids = set(range(num_centroids)) - set(aggregated_centroids.index)\n",
    "    for ix in missing_centroids:\n",
    "        # Copy old centroids to replace the missing\n",
    "        aggregated_centroids.loc[ix] = old_centroids[ix]\n",
    "    return aggregated_centroids.sort_index().to_numpy()\n",
    "\n",
    "def closest_centroids(data, centroids):\n",
    "    return data.mapPartitions(lambda partition: [calc_partition_centroid_means(partition, centroids)])\n",
    "\n",
    "def handle_missing_centorids(aggregated_centroids, old_centroids):\n",
    "    num_centorids = len(old_centroids)\n",
    "    missing_centroids = set(range(num_centorids)) - set(aggregated_centroids.index)\n",
    "    for ix in missing_centroids:\n",
    "        # Copy old centroids to replace the missing\n",
    "        print(\"handled missing\")\n",
    "        aggregated_centroids.loc[ix] = old_centroids[ix]\n",
    "    return aggregated_centroids.sort_index().to_numpy()\n",
    "\n",
    "def calc_error(new_centroids, old_centroids):\n",
    "    return np.sum(euclidean_dist(new_centroids, old_centroids))\n",
    "\n",
    "def pkmeans(data, n, max_iterations=150, stop_distance=0.001):\n",
    "    print(time.asctime(), \"Started\")\n",
    "    start_time = time.time()\n",
    "    init_centroids = np.array(data.takeSample(False, n, seed=42)) \n",
    "    centroids = sc.broadcast(init_centroids)\n",
    "\n",
    "    iteration = 1\n",
    "    error = float(\"inf\")\n",
    "    while error > stop_distance and iteration <= max_iterations:\n",
    "        loop_start = time.time()\n",
    "        closest_centroids_rdd = closest_centroids(data, centroids)\n",
    "        aggregated_centroids = aggregate_means(closest_centroids_rdd)\n",
    "        new_centroids = handle_missing_centorids(aggregated_centroids, centroids.value)\n",
    "        error = calc_error(new_centroids, centroids.value)\n",
    "        print(\"{3} Iteration #{0}\\tDistance between old and new centroids: {1:.4f}\\tIteration took: {2:.4f} sec\".format(\n",
    "            iteration, error, time.time() - loop_start, time.asctime()))\n",
    "        \n",
    "        # Update centroids\n",
    "        centroids = sc.broadcast(new_centroids) \n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"Total time: {time.time() - start_time}\")\n",
    "\n",
    "    return centroids.value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a91ef754-dbfa-4ffe-82ec-103b0d6f0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data.csv\", processed_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "065b785c-0c39-4c20-9fcf-7ec5244d2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PKmeans\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile(\"data.csv\")\n",
    "\n",
    "parsed_data = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "parsed_data = parsed_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2228c870-281a-45b1-8e45-61bea6f9a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:32 2025 Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:36 2025 Iteration #1\tDistance between old and new centroids: 36.8608\tIteration took: 0.8160 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:37 2025 Iteration #2\tDistance between old and new centroids: 0.3311\tIteration took: 0.6746 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:38 2025 Iteration #3\tDistance between old and new centroids: 0.2096\tIteration took: 0.6668 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:38 2025 Iteration #4\tDistance between old and new centroids: 0.1376\tIteration took: 0.7182 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:39 2025 Iteration #5\tDistance between old and new centroids: 0.0965\tIteration took: 0.7052 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:40 2025 Iteration #6\tDistance between old and new centroids: 0.0655\tIteration took: 0.6709 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:40 2025 Iteration #7\tDistance between old and new centroids: 0.0533\tIteration took: 0.6628 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:41 2025 Iteration #8\tDistance between old and new centroids: 0.0492\tIteration took: 0.6830 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:42 2025 Iteration #9\tDistance between old and new centroids: 0.0482\tIteration took: 0.7500 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:43 2025 Iteration #10\tDistance between old and new centroids: 0.0468\tIteration took: 0.6614 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:43 2025 Iteration #11\tDistance between old and new centroids: 0.0404\tIteration took: 0.6748 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:44 2025 Iteration #12\tDistance between old and new centroids: 0.0396\tIteration took: 0.7096 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:45 2025 Iteration #13\tDistance between old and new centroids: 0.0387\tIteration took: 0.6851 sec\n",
      "Thu Jan  2 18:17:45 2025 Iteration #14\tDistance between old and new centroids: 0.0400\tIteration took: 0.6516 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:46 2025 Iteration #15\tDistance between old and new centroids: 0.0395\tIteration took: 0.6600 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:47 2025 Iteration #16\tDistance between old and new centroids: 0.0389\tIteration took: 0.6682 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:47 2025 Iteration #17\tDistance between old and new centroids: 0.0380\tIteration took: 0.6869 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 18:17:48 2025 Iteration #18\tDistance between old and new centroids: 0.0353\tIteration took: 0.7090 sec\n",
      "Thu Jan  2 18:17:49 2025 Iteration #19\tDistance between old and new centroids: 0.0340\tIteration took: 0.6598 sec\n",
      "Thu Jan  2 18:17:49 2025 Iteration #20\tDistance between old and new centroids: 0.0334\tIteration took: 0.6481 sec\n",
      "Total time: 17.36097240447998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_centorids = pkmeans(parsed_data, k, max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "148a41c1-00a4-46b7-96a6-d95ba5184b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 18:17:54 WARN TaskSetManager: Stage 22 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/02 18:17:55 WARN TaskSetManager: Stage 23 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/02 18:17:58 WARN TaskSetManager: Stage 25 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 25:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.6177348314924771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labels = assign_clusters_to_data(processed_data, final_centorids)\n",
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e5f0fd64-2b72-4df6-8c70-33d5386a7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity score between two centroid groups.\n",
    "    The similarity is the inverse of the sum of distances between corresponding centroids.\n",
    "    \"\"\"\n",
    "    distances = [np.linalg.norm(c1 - c2) for c1, c2 in zip(group1, group2)]\n",
    "    return 1 / sum(distances) if sum(distances) != 0 else float('inf')\n",
    "\n",
    "\n",
    "def find_most_dissimilar_group(target_group, groups):\n",
    "    \"\"\"\n",
    "    Find the most dissimilar group to the target group.\n",
    "    \"\"\"\n",
    "    max_dissimilarity = -float('inf')\n",
    "    most_dissimilar = None\n",
    "    \n",
    "    for group in groups:\n",
    "        if np.array_equal(target_group, group):\n",
    "            continue  # Skip self-comparison\n",
    "        \n",
    "        similarity = calculate_similarity(target_group, group)\n",
    "        if similarity > max_dissimilarity:\n",
    "            max_dissimilarity = similarity\n",
    "            most_dissimilar = group\n",
    "    \n",
    "    return most_dissimilar\n",
    "\n",
    "\n",
    "def adgp(groups):\n",
    "    \"\"\"\n",
    "    Generate new centroid groups using the Average of Dissimilar Group Pairs (ADGP).\n",
    "    \"\"\"\n",
    "    new_groups = []\n",
    "    group_count = len(groups)\n",
    "    \n",
    "    for i, group1 in enumerate(groups):\n",
    "        group2 = find_most_dissimilar_group(group1, groups)\n",
    "        \n",
    "        # Compute the average of corresponding centroids to form a new group\n",
    "        new_group = [(c1 + c2) / 2 for c1, c2 in zip(group1, group2)]\n",
    "        new_groups.append(new_group)\n",
    "    \n",
    "    return new_groups\n",
    "\n",
    "# Permutation function to align centroids across groups\n",
    "def permute_centroids(centroid_groups):\n",
    "    base_group = centroid_groups[0]\n",
    "    permuted_groups = []\n",
    "\n",
    "    for group in centroid_groups[1:]:\n",
    "        # Track matched indices to prevent duplication\n",
    "        matched = set()\n",
    "        permuted_group = []\n",
    "        for base_c in base_group:\n",
    "            # Find the closest unmatched centroid\n",
    "            distances = [(i, np.linalg.norm(base_c - c)) for i, c in enumerate(group) if i not in matched]\n",
    "            if distances:\n",
    "                closest_idx = min(distances, key=lambda x: x[1])[0]\n",
    "                permuted_group.append(group[closest_idx])\n",
    "                matched.add(closest_idx)\n",
    "            else:\n",
    "                # Handle unmatched cases by using a default\n",
    "                permuted_group.append(base_c)\n",
    "        permuted_groups.append(permuted_group)\n",
    "    return [base_group] + permuted_groups\n",
    "\n",
    "def initialize_centroid_groups(parsed_data, k, s):\n",
    "    # Collect a small subset of the data for initialization\n",
    "    sample_data = parsed_data.takeSample(False, k * s, seed=1)\n",
    "    centroid_groups = [\n",
    "        sample_data[i * k:(i + 1) * k] for i in range(s)\n",
    "    ]\n",
    "    return np.array(centroid_groups)\n",
    "\n",
    "def mux_kmeans(data, k, s, max_iterations=20):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize centroid groups\n",
    "    initial_centroids_groups = initialize_centroid_groups(data, k, s)\n",
    "    \n",
    "    # Mux-Kmeans main loop\n",
    "    for iteration in range(max_iterations):\n",
    "        iteration_time = time.time()\n",
    "        twcv_scores = []\n",
    "        \n",
    "        # Evaluate centroid groups\n",
    "        for centroids in initial_centroids_groups:\n",
    "            centroids_broadcast = sc.broadcast(centroids)\n",
    "    \n",
    "            # Assign points to clusters\n",
    "            clustered_rdd = data.map(\n",
    "                lambda p: (\n",
    "                    np.argmin([np.linalg.norm(np.subtract(p, c)) for c in centroids_broadcast.value]),\n",
    "                    (p, 1)\n",
    "                )\n",
    "            ) # (Cluster index, (point, 1))\n",
    "\n",
    "            # Recalculate centroids\n",
    "            new_centroids = (\n",
    "                clustered_rdd\n",
    "                .reduceByKey(lambda x, y: (np.add(x[0], y[0]), x[1] + y[1]))  # Sum points and count\n",
    "                .map(lambda x: (x[0], np.array(x[1][0]) / x[1][1]))  # Compute new centroids\n",
    "                .collectAsMap()\n",
    "            )\n",
    "\n",
    "            new_centroids_arr = np.array(\n",
    "                    [new_centroids[j] if j in new_centroids else centroids[j] for j in range(len(centroids))]\n",
    "            )\n",
    "    \n",
    "            # Update the centroids to the new centroids\n",
    "            centroids_broadcast = sc.broadcast(new_centroids_arr)\n",
    "            \n",
    "            # Assign points to the nearest new centroids\n",
    "            centroid_to_point = data.map(\n",
    "                lambda point: (\n",
    "                    np.argmin([np.linalg.norm(point - c) for c in centroids_broadcast.value]), # cluster_id\n",
    "                    point # original point\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            # Calculate TWCV for the current group\n",
    "            # Row is (cluster_id, point)\n",
    "            twcv = centroid_to_point.map(lambda row: np.linalg.norm(row[1] - centroids_broadcast.value[row[0]]) ** 2).sum()\n",
    "            twcv_scores.append((new_centroids_arr, twcv))\n",
    "    \n",
    "        # Prune and incubate\n",
    "        twcv_scores.sort(key=lambda x: x[1])\n",
    "        best_groups = twcv_scores[:s // 2]\n",
    "        best_centroids = [x[0] for x in best_groups]\n",
    "    \n",
    "        # Permutation\n",
    "        aligned_centroid_groups = permute_centroids(best_centroids)\n",
    "        \n",
    "        # Incubate new groups\n",
    "        new_centroid_groups = []\n",
    "        for group in aligned_centroid_groups:\n",
    "            new_centroid_groups.append(adgp(group))\n",
    "    \n",
    "        # Prepare for the next iteration\n",
    "        initial_centroids_groups = best_centroids + new_centroid_groups\n",
    "        \n",
    "        best_twcv = min(twcv_scores, key=lambda x: x[1])[1]\n",
    "\n",
    "        print(f\"Iteration: {iteration}\\ttime taken: {time.time() - iteration_time:.4f} seconds\\tBest TWCV: {best_twcv}\")\n",
    "\n",
    "    # Select the final best group\n",
    "    final_group = min(twcv_scores, key=lambda x: x[1])\n",
    "    print(f\"Total time: {time.time() - start_time:.4f} seconds\")\n",
    "    return final_group[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bac5187e-16f4-45d1-a1cb-78b79cc4e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 18:17:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\ttime taken: 12.7545 seconds\tBest TWCV: 34882436.05236875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\ttime taken: 12.7229 seconds\tBest TWCV: 7155164.969586938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2\ttime taken: 13.1464 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3\ttime taken: 12.6756 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4\ttime taken: 12.4679 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5\ttime taken: 12.4873 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6\ttime taken: 12.3501 seconds\tBest TWCV: 4998941.539749891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7\ttime taken: 12.6354 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8\ttime taken: 12.8480 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9\ttime taken: 13.0941 seconds\tBest TWCV: 4998941.53974989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\ttime taken: 12.6050 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11\ttime taken: 12.9056 seconds\tBest TWCV: 4998941.539749891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12\ttime taken: 12.8852 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13\ttime taken: 12.7638 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14\ttime taken: 12.6284 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15\ttime taken: 12.4895 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16\ttime taken: 12.5426 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17\ttime taken: 12.7052 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18\ttime taken: 12.5687 seconds\tBest TWCV: 4998941.539749889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19\ttime taken: 12.4846 seconds\tBest TWCV: 4998941.539749889\n",
      "Total time: 254.9117 seconds\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mux-KMeans\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile(\"data.csv\")\n",
    "\n",
    "parsed_data = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "parsed_data = parsed_data.cache()\n",
    "final_centorids = mux_kmeans(parsed_data, k, s=6)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b85db287-9815-409a-9ae1-f3dcea27fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 18:22:20 WARN TaskSetManager: Stage 0 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/02 18:22:21 WARN TaskSetManager: Stage 1 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/02 18:22:25 WARN TaskSetManager: Stage 3 contains a task of very large size (5768 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.9686344960642669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2915a124 rejected from java.util.concurrent.ThreadPoolExecutor@2d2a20de[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 44]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mux-KMeans-Silhouette\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "labels = assign_clusters_to_data(processed_data, final_centorids)\n",
    "spark_df = kmeans_to_spark_df(processed_data, labels)\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
